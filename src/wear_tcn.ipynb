{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List\n",
    "\n",
    "from tensorflow.keras import backend as K, Model, Input, optimizers\n",
    "# pylint: disable=E0611\n",
    "from tensorflow.keras import layers, Sequential\n",
    "# pylint: disable=E0611\n",
    "from tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda\n",
    "# pylint: disable=E0611\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization, Reshape\n",
    "import tensorflow\n",
    "\n",
    "def is_power_of_two(num: int):\n",
    "    return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "\n",
    "def adjust_dilations(dilations: list):\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        return new_dilations\n",
    "\n",
    "class SE_Block(Layer):\n",
    "    def __init__(self,reduction = 16,**kwargs):\n",
    "        super(SE_Block,self).__init__(**kwargs)\n",
    "        tensorflow.config.run_functions_eagerly(True)\n",
    "        self.reduction = reduction\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    @tensorflow.function\n",
    "    def call(self, inputs):\n",
    "        num_filters = inputs.shape[-1]\n",
    "        squeeze = tensorflow.keras.layers.GlobalAveragePooling1D()(inputs)\n",
    "\n",
    "        excitation = tensorflow.keras.layers.Dense(units=num_filters/self.reduction)(squeeze)\n",
    "        excitation = tensorflow.keras.layers.Activation('relu')(excitation)\n",
    "        excitation = tensorflow.keras.layers.Dense(units=num_filters)(excitation)\n",
    "        excitation = tensorflow.keras.layers.Activation('sigmoid')(excitation)\n",
    "        excitation = tensorflow.keras.layers.Reshape([1, num_filters])(excitation)\n",
    "\n",
    "        scale = inputs * excitation\n",
    "\n",
    "        return scale\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dilation_rate: int,\n",
    "                 nb_filters: int,\n",
    "                 kernel_size: int,\n",
    "                 padding: str,\n",
    "                 activation: str = 'relu',\n",
    "                 dropout_rate: float = 0,\n",
    "                 kernel_initializer: str = 'he_normal',\n",
    "                 use_batch_norm: bool = False,\n",
    "                 use_layer_norm: bool = False,\n",
    "                 use_weight_norm: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"Defines the residual block for the WaveNet TCN\n",
    "        Args:\n",
    "            x: The previous layer in the model\n",
    "            training: boolean indicating whether the layer should behave in training mode or in inference mode\n",
    "            dilation_rate: The dilation power of 2 we are using for this residual block\n",
    "            nb_filters: The number of convolutional filters to use in this block\n",
    "            kernel_size: The size of the convolutional kernel\n",
    "            padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "            activation: The final activation used in o = Activation(x + F(x))\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n",
    "            use_weight_norm: Whether to use weight normalization in the residual layers or not.\n",
    "            kwargs: Any initializers for Layer class.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_weight_norm = use_weight_norm\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.layers = []\n",
    "        self.shape_match_conv = None\n",
    "        self.res_output_shape = None\n",
    "        self.final_activation = None\n",
    "\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "\n",
    "    def _build_layer(self, layer):\n",
    "        \"\"\"Helper function for building layer\n",
    "        Args:\n",
    "            layer: Appends layer to internal layer list and builds it based on the current output\n",
    "                   shape of ResidualBlocK. Updates current output shape.\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "        self.layers[-1].build(self.res_output_shape)\n",
    "        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        with K.name_scope(self.name):  # name scope used to make sure weights get unique names\n",
    "            self.layers = []\n",
    "            self.res_output_shape = input_shape\n",
    "\n",
    "            for k in range(2):  # dilated conv block.\n",
    "                name = 'conv1D_{}'.format(k)\n",
    "                with K.name_scope(name):  # name scope used to make sure weights get unique names\n",
    "                    conv = Conv1D(\n",
    "                        filters=self.nb_filters,\n",
    "                        kernel_size=self.kernel_size,\n",
    "                        dilation_rate=self.dilation_rate,\n",
    "                        padding=self.padding,\n",
    "                        name=name,\n",
    "                        kernel_initializer=self.kernel_initializer\n",
    "                    )\n",
    "                    if self.use_weight_norm:\n",
    "                        from tensorflow_addons.layers import WeightNormalization\n",
    "                        # wrap it. WeightNormalization API is different than BatchNormalization or LayerNormalization.\n",
    "                        with K.name_scope('norm_{}'.format(k)):\n",
    "                            conv = WeightNormalization(conv)\n",
    "                    self._build_layer(conv)\n",
    "\n",
    "                with K.name_scope('norm_{}'.format(k)):\n",
    "                    if self.use_batch_norm:\n",
    "                        self._build_layer(BatchNormalization())\n",
    "                    elif self.use_layer_norm:\n",
    "                        self._build_layer(LayerNormalization())\n",
    "                    elif self.use_weight_norm:\n",
    "                        pass  # done above.\n",
    "\n",
    "                with K.name_scope('act_and_dropout_{}'.format(k)):\n",
    "                    self._build_layer(Activation(self.activation, name='Act_Conv1D_{}'.format(k)))\n",
    "                    self._build_layer(SpatialDropout1D(rate=self.dropout_rate, name='SDropout_{}'.format(k)))\n",
    "\n",
    "            if self.nb_filters != input_shape[-1]:\n",
    "                # 1x1 conv to match the shapes (channel dimension).\n",
    "                name = 'matching_conv1D'\n",
    "                with K.name_scope(name):\n",
    "                    # make and build this layer separately because it directly uses input_shape.\n",
    "                    # 1x1 conv.\n",
    "                    self.shape_match_conv = Conv1D(\n",
    "                        filters=self.nb_filters,\n",
    "                        kernel_size=1,\n",
    "                        padding='same',\n",
    "                        name=name,\n",
    "                        kernel_initializer=self.kernel_initializer\n",
    "                    )\n",
    "            else:\n",
    "                name = 'matching_identity'\n",
    "                self.shape_match_conv = Lambda(lambda x: x, name=name)\n",
    "\n",
    "            with K.name_scope(name):\n",
    "                self.shape_match_conv.build(input_shape)\n",
    "                self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n",
    "\n",
    "            self._build_layer(Activation(self.activation, name='Act_Conv_Blocks'))\n",
    "            self.final_activation = Activation(self.activation, name='Act_Res_Block')\n",
    "            self.final_activation.build(self.res_output_shape)  # probably isn't necessary\n",
    "\n",
    "            # this is done to force Keras to add the layers in the list to self._layers\n",
    "            for layer in self.layers:\n",
    "                self.__setattr__(layer.name, layer)\n",
    "            self.__setattr__(self.shape_match_conv.name, self.shape_match_conv)\n",
    "            self.__setattr__(self.final_activation.name, self.final_activation)\n",
    "\n",
    "            super(ResidualBlock, self).build(input_shape)  # done to make sure self.built is set True\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns: A tuple where the first element is the residual model tensor, and the second\n",
    "                 is the skip connection tensor.\n",
    "        \"\"\"\n",
    "        # https://arxiv.org/pdf/1803.01271.pdf  page 4, Figure 1 (b).\n",
    "        # x1: Dilated Conv -> Norm -> Dropout (x2).\n",
    "        # x2: Residual (1x1 matching conv - optional).\n",
    "        # Output: x1 + x2.\n",
    "        # x1 -> connected to skip connections.\n",
    "        # x1 + x2 -> connected to the next block.\n",
    "        #       input\n",
    "        #     x1      x2\n",
    "        #   conv1D    1x1 Conv1D (optional)\n",
    "        #    ...\n",
    "        #   conv1D\n",
    "        #    ...\n",
    "        #       x1 + x2\n",
    "        x1 = inputs\n",
    "        for layer in self.layers:\n",
    "            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n",
    "            x1 = layer(x1, training=training) if training_flag else layer(x1)\n",
    "        x1 = SE_Block(reduction=16)(x1)\n",
    "\n",
    "        # print('x1.shape = ',x1.shape)\n",
    "        x2 = self.shape_match_conv(inputs)\n",
    "        x1_x2 = self.final_activation(layers.add([x2, x1], name='Add_Res'))\n",
    "        return [x1_x2, x1]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [self.res_output_shape, self.res_output_shape]\n",
    "\n",
    "\n",
    "class TCN(Layer):\n",
    "    \"\"\"Creates a TCN layer.\n",
    "        Input shape:\n",
    "            A tensor of shape (batch_size, timesteps, input_dim).\n",
    "        Args:\n",
    "            nb_filters: The number of filters to use in the convolutional layers. Can be a list.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            activation: The activation used in the residual blocks o = Activation(x + F(x)).\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n",
    "            use_weight_norm: Whether to use weight normalization in the residual layers or not.\n",
    "            kwargs: Any other arguments for configuring parent class Layer. For example \"name=str\", Name of the model.\n",
    "                    Use unique names when using multiple TCN.\n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_filters=32,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=(1, 2, 4, 8, 16),\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=False,\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 use_batch_norm=False,\n",
    "                 use_layer_norm=False,\n",
    "                 use_weight_norm=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.activation = activation\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_weight_norm = use_weight_norm\n",
    "        self.skip_connections = []\n",
    "        self.residual_blocks = []\n",
    "        self.layers_outputs = []\n",
    "        self.build_output_shape = None\n",
    "        self.slicer_layer = None  # in case return_sequence=False\n",
    "        self.output_slice_index = None  # in case return_sequence=False\n",
    "        self.padding_same_and_time_dim_unknown = False  # edge case if padding='same' and time_dim = None\n",
    "\n",
    "        if self.use_batch_norm + self.use_layer_norm + self.use_weight_norm > 1:\n",
    "            raise ValueError('Only one normalization can be specified at once.')\n",
    "\n",
    "        if isinstance(self.nb_filters, list):\n",
    "            assert len(self.nb_filters) == len(self.dilations)\n",
    "            if len(set(self.nb_filters)) > 1 and self.use_skip_connections:\n",
    "                raise ValueError('Skip connections are not compatible '\n",
    "                                 'with a list of filters, unless they are all equal.')\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
    "\n",
    "        # initialize parent class\n",
    "        super(TCN, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def receptive_field(self):\n",
    "        return 1 + 2 * (self.kernel_size - 1) * self.nb_stacks * sum(self.dilations)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # member to hold current output shape of the layer for building purposes\n",
    "        self.build_output_shape = input_shape\n",
    "\n",
    "        # list to hold all the member ResidualBlocks\n",
    "        self.residual_blocks = []\n",
    "        total_num_blocks = self.nb_stacks * len(self.dilations)\n",
    "        if not self.use_skip_connections:\n",
    "            total_num_blocks += 1  # cheap way to do a false case for below\n",
    "\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i, d in enumerate(self.dilations):\n",
    "                res_block_filters = self.nb_filters[i] if isinstance(self.nb_filters, list) else self.nb_filters\n",
    "                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n",
    "                                                          nb_filters=res_block_filters,\n",
    "                                                          kernel_size=self.kernel_size,\n",
    "                                                          padding=self.padding,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout_rate=self.dropout_rate,\n",
    "                                                          use_batch_norm=self.use_batch_norm,\n",
    "                                                          use_layer_norm=self.use_layer_norm,\n",
    "                                                          use_weight_norm=self.use_weight_norm,\n",
    "                                                          kernel_initializer=self.kernel_initializer,\n",
    "                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n",
    "                # build newest residual block\n",
    "                self.residual_blocks[-1].build(self.build_output_shape)\n",
    "                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n",
    "\n",
    "        # this is done to force keras to add the layers in the list to self._layers\n",
    "        for layer in self.residual_blocks:\n",
    "            self.__setattr__(layer.name, layer)\n",
    "\n",
    "        self.output_slice_index = None\n",
    "        if self.padding == 'same':\n",
    "            time = self.build_output_shape.as_list()[1]\n",
    "            if time is not None:  # if time dimension is defined. e.g. shape = (bs, 500, input_dim).\n",
    "                self.output_slice_index = int(self.build_output_shape.as_list()[1] / 2)\n",
    "            else:\n",
    "                # It will known at call time. c.f. self.call.\n",
    "                self.padding_same_and_time_dim_unknown = True\n",
    "\n",
    "        else:\n",
    "            self.output_slice_index = -1  # causal case.\n",
    "        self.slicer_layer = Lambda(lambda tt: tt[:, self.output_slice_index, :], name='Slice_Output')\n",
    "        self.slicer_layer.build(self.build_output_shape.as_list())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "        Overridden in case keras uses it somewhere... no idea. Just trying to avoid future errors.\n",
    "        \"\"\"\n",
    "        if not self.built:\n",
    "            self.build(input_shape)\n",
    "        if not self.return_sequences:\n",
    "            batch_size = self.build_output_shape[0]\n",
    "            batch_size = batch_size.value if hasattr(batch_size, 'value') else batch_size\n",
    "            nb_filters = self.build_output_shape[-1]\n",
    "            return [batch_size, nb_filters]\n",
    "        else:\n",
    "            # Compatibility tensorflow 1.x\n",
    "            return [v.value if hasattr(v, 'value') else v for v in self.build_output_shape]\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        x = inputs\n",
    "        self.layers_outputs = [x]\n",
    "        self.skip_connections = []\n",
    "        for res_block in self.residual_blocks:\n",
    "            try:\n",
    "                x, skip_out = res_block(x, training=training)\n",
    "            except TypeError:  # compatibility with tensorflow 1.x\n",
    "                x, skip_out = res_block(K.cast(x, 'float32'), training=training)\n",
    "            self.skip_connections.append(skip_out)\n",
    "            self.layers_outputs.append(x)\n",
    "\n",
    "        if self.use_skip_connections:\n",
    "            x = layers.add(self.skip_connections, name='Add_Skip_Connections')\n",
    "            self.layers_outputs.append(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            # case: time dimension is unknown. e.g. (bs, None, input_dim).\n",
    "            if self.padding_same_and_time_dim_unknown:\n",
    "                self.output_slice_index = K.shape(self.layers_outputs[-1])[1] // 2\n",
    "            x = self.slicer_layer(x)\n",
    "            self.layers_outputs.append(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the config of a the layer. This is used for saving and loading from a model\n",
    "        :return: python dictionary with specs to rebuild layer\n",
    "        \"\"\"\n",
    "        config = super(TCN, self).get_config()\n",
    "        config['nb_filters'] = self.nb_filters\n",
    "        config['kernel_size'] = self.kernel_size\n",
    "        config['nb_stacks'] = self.nb_stacks\n",
    "        config['dilations'] = self.dilations\n",
    "        config['padding'] = self.padding\n",
    "        config['use_skip_connections'] = self.use_skip_connections\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        config['return_sequences'] = self.return_sequences\n",
    "        config['activation'] = self.activation\n",
    "        config['use_batch_norm'] = self.use_batch_norm\n",
    "        config['use_layer_norm'] = self.use_layer_norm\n",
    "        config['use_weight_norm'] = self.use_weight_norm\n",
    "        config['kernel_initializer'] = self.kernel_initializer\n",
    "        return config\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def compiled_tcn(num_feat,  # type: int\n",
    "                 num_classes,  # type: int\n",
    "                 nb_filters,  # type: int\n",
    "                 kernel_size,  # type: int\n",
    "                 dilations,  # type: List[int]\n",
    "                 nb_stacks,  # type: int\n",
    "                 max_len,  # type: int\n",
    "                 output_len=1,  # type: int\n",
    "                 padding='causal',  # type: str\n",
    "                 use_skip_connections=True,  # type: bool\n",
    "                 return_sequences=False,\n",
    "                 regression=True,  # type: bool\n",
    "                 dropout_rate=0.05,  # type: float\n",
    "                 name='tcn',  # type: str,\n",
    "                 kernel_initializer='he_normal',  # type: str,\n",
    "                 activation='relu',  # type:str,\n",
    "                 opt='adam',\n",
    "                 learning_rate=0.002,\n",
    "                 use_batch_norm=False,\n",
    "                 use_layer_norm=False,\n",
    "                 use_weight_norm=False):\n",
    "    # type: (...) -> Model\n",
    "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
    "    Classification uses a sparse categorical loss. Please input class ids and not one-hot encodings.\n",
    "    Args:\n",
    "        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n",
    "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
    "        nb_filters: The number of filters to use in the convolutional layers.\n",
    "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "        nb_stacks : The number of stacks of residual blocks to use.\n",
    "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
    "        padding: The padding to use in the convolutional layers.\n",
    "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n",
    "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "        regression: Whether the output should be continuous or discrete.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        activation: The activation used in the residual blocks o = Activation(x + F(x)).\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "        kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "        opt: Optimizer name.\n",
    "        lr: Learning rate.\n",
    "        use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "        use_layer_norm: Whether to use layer normalization in the residual layers or not.\n",
    "        use_weight_norm: Whether to use weight normalization in the residual layers or not.\n",
    "    Returns:\n",
    "        A compiled keras TCN.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    dilations = adjust_dilations(dilations)\n",
    "\n",
    "    input_layer = Input(shape=(max_len, num_feat))\n",
    "\n",
    "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, padding,\n",
    "            use_skip_connections, dropout_rate, return_sequences,\n",
    "            activation, kernel_initializer, use_batch_norm, use_layer_norm,\n",
    "            use_weight_norm, name=name)(input_layer)\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "    def get_opt():\n",
    "        if opt == 'adam':\n",
    "            return optimizers.Adam(learning_rate=learning_rate, clipnorm=1.)\n",
    "        elif opt == 'rmsprop':\n",
    "            return optimizers.RMSprop(learning_rate=learning_rate, clipnorm=1.)\n",
    "        else:\n",
    "            raise Exception('Only Adam and RMSProp are available here')\n",
    "\n",
    "    if not regression:\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        # x = Reshape((1, -1))(x)\n",
    "        # x = Conv1D(filters=output_len, kernel_size=1,activation='softmax', name='test-conv')(x)\n",
    "\n",
    "        output_layer = x\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO remove later.\n",
    "        def accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            if K.ndim(y_true) == K.ndim(y_pred):\n",
    "                y_true = K.squeeze(y_true, -1)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"]) # sparse_categorical_crossentropy [1] [2]\n",
    "    else:\n",
    "        # regression\n",
    "        x = Dense(output_len)(x)\n",
    "        x = Activation('linear')(x)\n",
    "\n",
    "\n",
    "        output_layer = x\n",
    "        model = Model(input_layer, output_layer)\n",
    "        model.compile(get_opt(), loss='mse', metrics=['mse',\"logcosh\", \"acc\"])\n",
    "    print('model.x = {}'.format(input_layer.shape))\n",
    "    print('model.y = {}'.format(output_layer.shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def tcn_full_summary(model: Model, expand_residual_blocks=True):\n",
    "    import tensorflow as tf\n",
    "    # 2.6.0-rc1, 2.5.0...\n",
    "    versions = [int(v) for v in tf.__version__.split('-')[0].split('.')]\n",
    "    if versions[0] <= 2 and versions[1] < 5:\n",
    "        layers = model._layers.copy()  # store existing layers\n",
    "        model._layers.clear()  # clear layers\n",
    "\n",
    "        for i in range(len(layers)):\n",
    "            if isinstance(layers[i], TCN):\n",
    "                for layer in layers[i]._layers:\n",
    "                    if not isinstance(layer, ResidualBlock):\n",
    "                        if not hasattr(layer, '__iter__'):\n",
    "                            model._layers.append(layer)\n",
    "                    else:\n",
    "                        if expand_residual_blocks:\n",
    "                            for lyr in layer._layers:\n",
    "                                if not hasattr(lyr, '__iter__'):\n",
    "                                    model._layers.append(lyr)\n",
    "                        else:\n",
    "                            model._layers.append(layer)\n",
    "            else:\n",
    "                model._layers.append(layers[i])\n",
    "\n",
    "        model.summary()  # print summary\n",
    "\n",
    "        # restore original layers\n",
    "        model._layers.clear()\n",
    "        [model._layers.append(lyr) for lyr in layers]\n",
    "    else:\n",
    "        print('WARNING: tcn_full_summary: Compatible with tensorflow 2.5.0 or below.')\n",
    "        print('Use tensorboard instead. Example in keras-tcn/tasks/tcn_tensorboard.py.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data( data, scale_method = MinMaxScaler(feature_range=(0, 1)), verbose = True):\n",
    "    scaler = scale_method\n",
    "    series = scaler.fit_transform(data)\n",
    "    if verbose :\n",
    "        import plotly.graph_objects as go\n",
    "\n",
    "        line = go.Scatter(y = data[:, 0][:10240], name = \"原始齒輪振動資料\")\n",
    "        init_data = go.Scatter(y = series[:, 0][:10240], name = \"特徵處理後的資料\")\n",
    "        fig = go.Figure([line, init_data])\n",
    "        fig.show()\n",
    "    return scaler, series"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "filepath = 'C:/Users/user/Desktop/TCN/資料擷取/資料擷取_npy'\n",
    "\n",
    "data_len = 5000\n",
    "GOOD = (numpy.load(f'{filepath}/top/Good.npy'))\n",
    "wear_top = (numpy.load(f'{filepath}/top/Wear.npy'))\n",
    "wear_mid = (numpy.load(f'{filepath}/mid/Wear.npy'))\n",
    "wear_bot = (numpy.load(f'{filepath}/bot/Wear.npy'))\n",
    "\n",
    "\n",
    "\n",
    "datas_list_wear = [GOOD.reshape(-1,2048,3)[:data_len], wear_top.reshape(-1,2048,3)[:data_len] , wear_bot.reshape(-1,2048,3)[:data_len]]\n",
    "datas_list_wear = numpy.concatenate(datas_list_wear)\n",
    "\n",
    "wear_label = [0.0 for i in range(data_len)]+ [0.0625 for i in range(data_len)]  + [0.2017 for i in range(data_len)]\n",
    "wear_label = numpy.asarray(wear_label)\n",
    "print(len(wear_label))\n",
    "\n",
    "p = numpy.random.permutation(len(wear_label))\n",
    "datas_list_wear = datas_list_wear[p]\n",
    "wear_label = wear_label[p]\n",
    "\n",
    "# wear_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape= (None, 128)\n",
      "model.x = (None, None, 3)\n",
      "model.y = (None, 1)\n",
      "Epoch 1/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 0.0535 - mse: 0.0535 - logcosh: 0.0218 - acc: 0.3336 - val_loss: 0.0160 - val_mse: 0.0160 - val_logcosh: 0.0077 - val_acc: 0.3293\n",
      "Epoch 2/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0137 - mse: 0.0137 - logcosh: 0.0066 - acc: 0.3338 - val_loss: 0.0083 - val_mse: 0.0083 - val_logcosh: 0.0041 - val_acc: 0.3293\n",
      "Epoch 3/100\n",
      "422/422 [==============================] - 35s 84ms/step - loss: 0.0079 - mse: 0.0079 - logcosh: 0.0039 - acc: 0.3338 - val_loss: 0.0058 - val_mse: 0.0058 - val_logcosh: 0.0029 - val_acc: 0.3293\n",
      "Epoch 4/100\n",
      "422/422 [==============================] - 35s 82ms/step - loss: 0.0056 - mse: 0.0056 - logcosh: 0.0027 - acc: 0.3338 - val_loss: 0.0045 - val_mse: 0.0045 - val_logcosh: 0.0022 - val_acc: 0.3293\n",
      "Epoch 5/100\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 0.0045 - mse: 0.0045 - logcosh: 0.0022 - acc: 0.3338 - val_loss: 0.0040 - val_mse: 0.0040 - val_logcosh: 0.0020 - val_acc: 0.3293\n",
      "Epoch 6/100\n",
      "422/422 [==============================] - 35s 82ms/step - loss: 0.0038 - mse: 0.0038 - logcosh: 0.0019 - acc: 0.3338 - val_loss: 0.0038 - val_mse: 0.0038 - val_logcosh: 0.0019 - val_acc: 0.3293\n",
      "Epoch 7/100\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0034 - mse: 0.0034 - logcosh: 0.0017 - acc: 0.3338 - val_loss: 0.0034 - val_mse: 0.0034 - val_logcosh: 0.0017 - val_acc: 0.3293\n",
      "Epoch 8/100\n",
      "422/422 [==============================] - 35s 82ms/step - loss: 0.0030 - mse: 0.0030 - logcosh: 0.0015 - acc: 0.3338 - val_loss: 0.0031 - val_mse: 0.0031 - val_logcosh: 0.0016 - val_acc: 0.3293\n",
      "Epoch 9/100\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0028 - mse: 0.0028 - logcosh: 0.0014 - acc: 0.3338 - val_loss: 0.0029 - val_mse: 0.0029 - val_logcosh: 0.0015 - val_acc: 0.3293\n",
      "Epoch 10/100\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0026 - mse: 0.0026 - logcosh: 0.0013 - acc: 0.3338 - val_loss: 0.0027 - val_mse: 0.0027 - val_logcosh: 0.0013 - val_acc: 0.3293\n",
      "Epoch 11/100\n",
      "422/422 [==============================] - 35s 82ms/step - loss: 0.0024 - mse: 0.0024 - logcosh: 0.0012 - acc: 0.3338 - val_loss: 0.0026 - val_mse: 0.0026 - val_logcosh: 0.0013 - val_acc: 0.3293\n",
      "Epoch 12/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 0.0021 - mse: 0.0021 - logcosh: 0.0011 - acc: 0.3338 - val_loss: 0.0024 - val_mse: 0.0024 - val_logcosh: 0.0012 - val_acc: 0.3293\n",
      "Epoch 13/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0020 - mse: 0.0020 - logcosh: 0.0010 - acc: 0.3338 - val_loss: 0.0023 - val_mse: 0.0023 - val_logcosh: 0.0011 - val_acc: 0.3293\n",
      "Epoch 14/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0019 - mse: 0.0019 - logcosh: 9.4076e-04 - acc: 0.3338 - val_loss: 0.0022 - val_mse: 0.0022 - val_logcosh: 0.0011 - val_acc: 0.3293\n",
      "Epoch 15/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0017 - mse: 0.0017 - logcosh: 8.5231e-04 - acc: 0.3338 - val_loss: 0.0021 - val_mse: 0.0021 - val_logcosh: 0.0011 - val_acc: 0.3293\n",
      "Epoch 16/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 0.0016 - mse: 0.0016 - logcosh: 7.9884e-04 - acc: 0.3338 - val_loss: 0.0020 - val_mse: 0.0020 - val_logcosh: 0.0010 - val_acc: 0.3293\n",
      "Epoch 17/100\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 0.0015 - mse: 0.0015 - logcosh: 7.3156e-04 - acc: 0.3338 - val_loss: 0.0019 - val_mse: 0.0019 - val_logcosh: 9.5463e-04 - val_acc: 0.3293\n",
      "Epoch 18/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 0.0014 - mse: 0.0014 - logcosh: 6.8173e-04 - acc: 0.3338 - val_loss: 0.0018 - val_mse: 0.0018 - val_logcosh: 8.9823e-04 - val_acc: 0.3293\n",
      "Epoch 19/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 0.0013 - mse: 0.0013 - logcosh: 6.3853e-04 - acc: 0.3338 - val_loss: 0.0018 - val_mse: 0.0018 - val_logcosh: 8.9773e-04 - val_acc: 0.3293\n",
      "Epoch 20/100\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 0.0012 - mse: 0.0012 - logcosh: 5.9831e-04 - acc: 0.3338 - val_loss: 0.0017 - val_mse: 0.0017 - val_logcosh: 8.4382e-04 - val_acc: 0.3293\n",
      "Epoch 21/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 0.0012 - mse: 0.0012 - logcosh: 5.7538e-04 - acc: 0.3338 - val_loss: 0.0017 - val_mse: 0.0017 - val_logcosh: 8.4258e-04 - val_acc: 0.3293\n",
      "Epoch 22/100\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 0.0011 - mse: 0.0011 - logcosh: 5.3308e-04 - acc: 0.3338 - val_loss: 0.0016 - val_mse: 0.0016 - val_logcosh: 8.0154e-04 - val_acc: 0.3293\n",
      "Epoch 23/100\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 9.9834e-04 - mse: 9.9834e-04 - logcosh: 4.9880e-04 - acc: 0.3338 - val_loss: 0.0015 - val_mse: 0.0015 - val_logcosh: 7.6567e-04 - val_acc: 0.3293\n",
      "Epoch 24/100\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 9.4291e-04 - mse: 9.4291e-04 - logcosh: 4.7112e-04 - acc: 0.3338 - val_loss: 0.0015 - val_mse: 0.0015 - val_logcosh: 7.6361e-04 - val_acc: 0.3293\n",
      "Epoch 25/100\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 8.8654e-04 - mse: 8.8654e-04 - logcosh: 4.4297e-04 - acc: 0.3338 - val_loss: 0.0016 - val_mse: 0.0016 - val_logcosh: 7.8353e-04 - val_acc: 0.3293\n",
      "Epoch 26/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 8.4290e-04 - mse: 8.4290e-04 - logcosh: 4.2116e-04 - acc: 0.3338 - val_loss: 0.0015 - val_mse: 0.0015 - val_logcosh: 7.2939e-04 - val_acc: 0.3293\n",
      "Epoch 27/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 8.0067e-04 - mse: 8.0067e-04 - logcosh: 4.0006e-04 - acc: 0.3338 - val_loss: 0.0014 - val_mse: 0.0014 - val_logcosh: 6.8986e-04 - val_acc: 0.3293\n",
      "Epoch 28/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 7.7013e-04 - mse: 7.7013e-04 - logcosh: 3.8482e-04 - acc: 0.3338 - val_loss: 0.0014 - val_mse: 0.0014 - val_logcosh: 7.0026e-04 - val_acc: 0.3293\n",
      "Epoch 29/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 7.4321e-04 - mse: 7.4321e-04 - logcosh: 3.7138e-04 - acc: 0.3338 - val_loss: 0.0014 - val_mse: 0.0014 - val_logcosh: 7.1325e-04 - val_acc: 0.3293\n",
      "Epoch 30/100\n",
      "422/422 [==============================] - 35s 82ms/step - loss: 6.6921e-04 - mse: 6.6921e-04 - logcosh: 3.3443e-04 - acc: 0.3338 - val_loss: 0.0013 - val_mse: 0.0013 - val_logcosh: 6.7278e-04 - val_acc: 0.3293\n",
      "Epoch 31/100\n",
      "422/422 [==============================] - 35s 84ms/step - loss: 6.4316e-04 - mse: 6.4316e-04 - logcosh: 3.2141e-04 - acc: 0.3338 - val_loss: 0.0013 - val_mse: 0.0013 - val_logcosh: 6.6869e-04 - val_acc: 0.3293\n",
      "Epoch 32/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 6.2617e-04 - mse: 6.2617e-04 - logcosh: 3.1292e-04 - acc: 0.3338 - val_loss: 0.0013 - val_mse: 0.0013 - val_logcosh: 6.5166e-04 - val_acc: 0.3293\n",
      "Epoch 33/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 5.9754e-04 - mse: 5.9754e-04 - logcosh: 2.9862e-04 - acc: 0.3338 - val_loss: 0.0013 - val_mse: 0.0013 - val_logcosh: 6.5153e-04 - val_acc: 0.3293\n",
      "Epoch 34/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 5.7394e-04 - mse: 5.7394e-04 - logcosh: 2.8684e-04 - acc: 0.3338 - val_loss: 0.0012 - val_mse: 0.0012 - val_logcosh: 6.2270e-04 - val_acc: 0.3293\n",
      "Epoch 35/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 5.4099e-04 - mse: 5.4099e-04 - logcosh: 2.7037e-04 - acc: 0.3338 - val_loss: 0.0012 - val_mse: 0.0012 - val_logcosh: 6.0957e-04 - val_acc: 0.3293\n",
      "Epoch 36/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 5.2569e-04 - mse: 5.2569e-04 - logcosh: 2.6273e-04 - acc: 0.3338 - val_loss: 0.0012 - val_mse: 0.0012 - val_logcosh: 6.0599e-04 - val_acc: 0.3293\n",
      "Epoch 37/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 5.0103e-04 - mse: 5.0103e-04 - logcosh: 2.5042e-04 - acc: 0.3338 - val_loss: 0.0012 - val_mse: 0.0012 - val_logcosh: 6.2199e-04 - val_acc: 0.3293\n",
      "Epoch 38/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 4.7634e-04 - mse: 4.7634e-04 - logcosh: 2.3808e-04 - acc: 0.3338 - val_loss: 0.0012 - val_mse: 0.0012 - val_logcosh: 5.8984e-04 - val_acc: 0.3293\n",
      "Epoch 39/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 4.6022e-04 - mse: 4.6022e-04 - logcosh: 2.3002e-04 - acc: 0.3338 - val_loss: 0.0012 - val_mse: 0.0012 - val_logcosh: 5.7441e-04 - val_acc: 0.3293\n",
      "Epoch 40/100\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 4.4055e-04 - mse: 4.4055e-04 - logcosh: 2.2020e-04 - acc: 0.3338 - val_loss: 0.0012 - val_mse: 0.0012 - val_logcosh: 6.0365e-04 - val_acc: 0.3293\n",
      "Epoch 41/100\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 4.2355e-04 - mse: 4.2355e-04 - logcosh: 2.1170e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.6593e-04 - val_acc: 0.3293\n",
      "Epoch 42/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 4.0985e-04 - mse: 4.0985e-04 - logcosh: 2.0485e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.4268e-04 - val_acc: 0.3293\n",
      "Epoch 43/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 3.8946e-04 - mse: 3.8946e-04 - logcosh: 1.9467e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.6469e-04 - val_acc: 0.3293\n",
      "Epoch 44/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 3.7375e-04 - mse: 3.7375e-04 - logcosh: 1.8682e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.4887e-04 - val_acc: 0.3293\n",
      "Epoch 45/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 3.6154e-04 - mse: 3.6154e-04 - logcosh: 1.8071e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.6248e-04 - val_acc: 0.3293\n",
      "Epoch 46/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 3.4366e-04 - mse: 3.4366e-04 - logcosh: 1.7178e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.3048e-04 - val_acc: 0.3293\n",
      "Epoch 47/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 3.4252e-04 - mse: 3.4252e-04 - logcosh: 1.7121e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.4102e-04 - val_acc: 0.3293\n",
      "Epoch 48/100\n",
      "422/422 [==============================] - 35s 84ms/step - loss: 3.3683e-04 - mse: 3.3683e-04 - logcosh: 1.6837e-04 - acc: 0.3338 - val_loss: 0.0011 - val_mse: 0.0011 - val_logcosh: 5.2706e-04 - val_acc: 0.3293\n",
      "Epoch 49/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 3.1054e-04 - mse: 3.1054e-04 - logcosh: 1.5523e-04 - acc: 0.3338 - val_loss: 0.0010 - val_mse: 0.0010 - val_logcosh: 5.0472e-04 - val_acc: 0.3293\n",
      "Epoch 50/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 3.0405e-04 - mse: 3.0405e-04 - logcosh: 1.5199e-04 - acc: 0.3338 - val_loss: 0.0010 - val_mse: 0.0010 - val_logcosh: 5.0889e-04 - val_acc: 0.3293\n",
      "Epoch 51/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 3.0030e-04 - mse: 3.0030e-04 - logcosh: 1.5011e-04 - acc: 0.3338 - val_loss: 9.6119e-04 - val_mse: 9.6119e-04 - val_logcosh: 4.8009e-04 - val_acc: 0.3293\n",
      "Epoch 52/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 2.9634e-04 - mse: 2.9634e-04 - logcosh: 1.4813e-04 - acc: 0.3338 - val_loss: 9.7301e-04 - val_mse: 9.7301e-04 - val_logcosh: 4.8601e-04 - val_acc: 0.3293\n",
      "Epoch 53/100\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 2.8840e-04 - mse: 2.8840e-04 - logcosh: 1.4416e-04 - acc: 0.3338 - val_loss: 9.8493e-04 - val_mse: 9.8493e-04 - val_logcosh: 4.9197e-04 - val_acc: 0.3293\n",
      "Epoch 54/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 2.6819e-04 - mse: 2.6819e-04 - logcosh: 1.3406e-04 - acc: 0.3338 - val_loss: 9.8356e-04 - val_mse: 9.8356e-04 - val_logcosh: 4.9123e-04 - val_acc: 0.3293\n",
      "Epoch 55/100\n",
      "422/422 [==============================] - 35s 84ms/step - loss: 2.7404e-04 - mse: 2.7404e-04 - logcosh: 1.3699e-04 - acc: 0.3338 - val_loss: 9.4205e-04 - val_mse: 9.4205e-04 - val_logcosh: 4.7053e-04 - val_acc: 0.3293\n",
      "Epoch 56/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 2.6471e-04 - mse: 2.6471e-04 - logcosh: 1.3232e-04 - acc: 0.3338 - val_loss: 9.2717e-04 - val_mse: 9.2717e-04 - val_logcosh: 4.6311e-04 - val_acc: 0.3293\n",
      "Epoch 57/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 2.4736e-04 - mse: 2.4736e-04 - logcosh: 1.2365e-04 - acc: 0.3338 - val_loss: 9.5580e-04 - val_mse: 9.5580e-04 - val_logcosh: 4.7741e-04 - val_acc: 0.3293\n",
      "Epoch 58/100\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 2.3929e-04 - mse: 2.3929e-04 - logcosh: 1.1962e-04 - acc: 0.3338 - val_loss: 9.4281e-04 - val_mse: 9.4281e-04 - val_logcosh: 4.7089e-04 - val_acc: 0.3293\n",
      "Epoch 59/100\n",
      "422/422 [==============================] - 35s 84ms/step - loss: 2.4336e-04 - mse: 2.4336e-04 - logcosh: 1.2165e-04 - acc: 0.3338 - val_loss: 9.4893e-04 - val_mse: 9.4893e-04 - val_logcosh: 4.7396e-04 - val_acc: 0.3293\n",
      "Epoch 60/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 2.3692e-04 - mse: 2.3692e-04 - logcosh: 1.1843e-04 - acc: 0.3338 - val_loss: 9.0430e-04 - val_mse: 9.0430e-04 - val_logcosh: 4.5166e-04 - val_acc: 0.3293\n",
      "Epoch 61/100\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 2.2159e-04 - mse: 2.2159e-04 - logcosh: 1.1077e-04 - acc: 0.3338 - val_loss: 8.8406e-04 - val_mse: 8.8406e-04 - val_logcosh: 4.4157e-04 - val_acc: 0.3293\n",
      "Epoch 62/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 2.2494e-04 - mse: 2.2494e-04 - logcosh: 1.1245e-04 - acc: 0.3338 - val_loss: 8.8462e-04 - val_mse: 8.8462e-04 - val_logcosh: 4.4186e-04 - val_acc: 0.3293\n",
      "Epoch 63/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 2.1978e-04 - mse: 2.1978e-04 - logcosh: 1.0987e-04 - acc: 0.3338 - val_loss: 8.7214e-04 - val_mse: 8.7214e-04 - val_logcosh: 4.3560e-04 - val_acc: 0.3293\n",
      "Epoch 64/100\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 2.0903e-04 - mse: 2.0903e-04 - logcosh: 1.0449e-04 - acc: 0.3338 - val_loss: 8.6147e-04 - val_mse: 8.6147e-04 - val_logcosh: 4.3029e-04 - val_acc: 0.3293\n",
      "Epoch 65/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 2.1224e-04 - mse: 2.1224e-04 - logcosh: 1.0610e-04 - acc: 0.3338 - val_loss: 8.3806e-04 - val_mse: 8.3806e-04 - val_logcosh: 4.1861e-04 - val_acc: 0.3293\n",
      "Epoch 66/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 2.0185e-04 - mse: 2.0185e-04 - logcosh: 1.0091e-04 - acc: 0.3338 - val_loss: 8.3604e-04 - val_mse: 8.3604e-04 - val_logcosh: 4.1757e-04 - val_acc: 0.3293\n",
      "Epoch 67/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 1.9691e-04 - mse: 1.9691e-04 - logcosh: 9.8438e-05 - acc: 0.3338 - val_loss: 8.3440e-04 - val_mse: 8.3440e-04 - val_logcosh: 4.1677e-04 - val_acc: 0.3293\n",
      "Epoch 68/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 2.0118e-04 - mse: 2.0118e-04 - logcosh: 1.0057e-04 - acc: 0.3338 - val_loss: 8.0536e-04 - val_mse: 8.0536e-04 - val_logcosh: 4.0228e-04 - val_acc: 0.3293\n",
      "Epoch 69/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.9253e-04 - mse: 1.9253e-04 - logcosh: 9.6246e-05 - acc: 0.3338 - val_loss: 7.9397e-04 - val_mse: 7.9397e-04 - val_logcosh: 3.9656e-04 - val_acc: 0.3293\n",
      "Epoch 70/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.9680e-04 - mse: 1.9680e-04 - logcosh: 9.8360e-05 - acc: 0.3338 - val_loss: 8.0372e-04 - val_mse: 8.0372e-04 - val_logcosh: 4.0145e-04 - val_acc: 0.3293\n",
      "Epoch 71/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.8812e-04 - mse: 1.8812e-04 - logcosh: 9.4043e-05 - acc: 0.3338 - val_loss: 7.8467e-04 - val_mse: 7.8467e-04 - val_logcosh: 3.9192e-04 - val_acc: 0.3293\n",
      "Epoch 72/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 1.8098e-04 - mse: 1.8098e-04 - logcosh: 9.0475e-05 - acc: 0.3338 - val_loss: 7.6332e-04 - val_mse: 7.6332e-04 - val_logcosh: 3.8127e-04 - val_acc: 0.3293\n",
      "Epoch 73/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 1.8208e-04 - mse: 1.8208e-04 - logcosh: 9.1022e-05 - acc: 0.3338 - val_loss: 7.6361e-04 - val_mse: 7.6361e-04 - val_logcosh: 3.8141e-04 - val_acc: 0.3293\n",
      "Epoch 74/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.7887e-04 - mse: 1.7887e-04 - logcosh: 8.9417e-05 - acc: 0.3338 - val_loss: 8.0233e-04 - val_mse: 8.0233e-04 - val_logcosh: 4.0072e-04 - val_acc: 0.3293\n",
      "Epoch 75/100\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 1.7288e-04 - mse: 1.7288e-04 - logcosh: 8.6425e-05 - acc: 0.3338 - val_loss: 7.9722e-04 - val_mse: 7.9722e-04 - val_logcosh: 3.9817e-04 - val_acc: 0.3293\n",
      "Epoch 76/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.6686e-04 - mse: 1.6686e-04 - logcosh: 8.3417e-05 - acc: 0.3338 - val_loss: 7.7229e-04 - val_mse: 7.7229e-04 - val_logcosh: 3.8575e-04 - val_acc: 0.3293\n",
      "Epoch 77/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.5880e-04 - mse: 1.5880e-04 - logcosh: 7.9386e-05 - acc: 0.3338 - val_loss: 7.5098e-04 - val_mse: 7.5098e-04 - val_logcosh: 3.7506e-04 - val_acc: 0.3293\n",
      "Epoch 78/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.5413e-04 - mse: 1.5413e-04 - logcosh: 7.7050e-05 - acc: 0.3338 - val_loss: 7.4312e-04 - val_mse: 7.4312e-04 - val_logcosh: 3.7117e-04 - val_acc: 0.3293\n",
      "Epoch 79/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.5673e-04 - mse: 1.5673e-04 - logcosh: 7.8352e-05 - acc: 0.3338 - val_loss: 7.6314e-04 - val_mse: 7.6314e-04 - val_logcosh: 3.8117e-04 - val_acc: 0.3293\n",
      "Epoch 80/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.6382e-04 - mse: 1.6382e-04 - logcosh: 8.1897e-05 - acc: 0.3338 - val_loss: 7.6471e-04 - val_mse: 7.6471e-04 - val_logcosh: 3.8194e-04 - val_acc: 0.3293\n",
      "Epoch 81/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.5320e-04 - mse: 1.5320e-04 - logcosh: 7.6586e-05 - acc: 0.3338 - val_loss: 7.0880e-04 - val_mse: 7.0880e-04 - val_logcosh: 3.5403e-04 - val_acc: 0.3293\n",
      "Epoch 82/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.5634e-04 - mse: 1.5634e-04 - logcosh: 7.8157e-05 - acc: 0.3338 - val_loss: 7.2247e-04 - val_mse: 7.2247e-04 - val_logcosh: 3.6083e-04 - val_acc: 0.3293\n",
      "Epoch 83/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.5366e-04 - mse: 1.5366e-04 - logcosh: 7.6817e-05 - acc: 0.3338 - val_loss: 7.0188e-04 - val_mse: 7.0188e-04 - val_logcosh: 3.5056e-04 - val_acc: 0.3293\n",
      "Epoch 84/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.4827e-04 - mse: 1.4827e-04 - logcosh: 7.4123e-05 - acc: 0.3338 - val_loss: 7.0902e-04 - val_mse: 7.0902e-04 - val_logcosh: 3.5411e-04 - val_acc: 0.3293\n",
      "Epoch 85/100\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 1.4471e-04 - mse: 1.4471e-04 - logcosh: 7.2347e-05 - acc: 0.3338 - val_loss: 6.8946e-04 - val_mse: 6.8946e-04 - val_logcosh: 3.4437e-04 - val_acc: 0.3293\n",
      "Epoch 86/100\n",
      "422/422 [==============================] - 35s 82ms/step - loss: 1.3864e-04 - mse: 1.3864e-04 - logcosh: 6.9310e-05 - acc: 0.3338 - val_loss: 6.8082e-04 - val_mse: 6.8082e-04 - val_logcosh: 3.4005e-04 - val_acc: 0.3293\n",
      "Epoch 87/100\n",
      "422/422 [==============================] - 35s 84ms/step - loss: 1.3862e-04 - mse: 1.3862e-04 - logcosh: 6.9299e-05 - acc: 0.3338 - val_loss: 6.8047e-04 - val_mse: 6.8047e-04 - val_logcosh: 3.3985e-04 - val_acc: 0.3293\n",
      "Epoch 88/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.3976e-04 - mse: 1.3976e-04 - logcosh: 6.9871e-05 - acc: 0.3338 - val_loss: 6.8830e-04 - val_mse: 6.8830e-04 - val_logcosh: 3.4380e-04 - val_acc: 0.3293\n",
      "Epoch 89/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.3663e-04 - mse: 1.3663e-04 - logcosh: 6.8308e-05 - acc: 0.3338 - val_loss: 6.6131e-04 - val_mse: 6.6131e-04 - val_logcosh: 3.3029e-04 - val_acc: 0.3293\n",
      "Epoch 90/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.3768e-04 - mse: 1.3768e-04 - logcosh: 6.8829e-05 - acc: 0.3338 - val_loss: 6.9939e-04 - val_mse: 6.9939e-04 - val_logcosh: 3.4931e-04 - val_acc: 0.3293\n",
      "Epoch 91/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.3359e-04 - mse: 1.3359e-04 - logcosh: 6.6783e-05 - acc: 0.3338 - val_loss: 6.5231e-04 - val_mse: 6.5231e-04 - val_logcosh: 3.2580e-04 - val_acc: 0.3293\n",
      "Epoch 92/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 1.3840e-04 - mse: 1.3840e-04 - logcosh: 6.9188e-05 - acc: 0.3338 - val_loss: 6.7141e-04 - val_mse: 6.7141e-04 - val_logcosh: 3.3535e-04 - val_acc: 0.3293\n",
      "Epoch 93/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.3009e-04 - mse: 1.3009e-04 - logcosh: 6.5038e-05 - acc: 0.3338 - val_loss: 6.4309e-04 - val_mse: 6.4309e-04 - val_logcosh: 3.2122e-04 - val_acc: 0.3293\n",
      "Epoch 94/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.2557e-04 - mse: 1.2557e-04 - logcosh: 6.2776e-05 - acc: 0.3338 - val_loss: 6.4202e-04 - val_mse: 6.4202e-04 - val_logcosh: 3.2065e-04 - val_acc: 0.3293\n",
      "Epoch 95/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.2583e-04 - mse: 1.2583e-04 - logcosh: 6.2905e-05 - acc: 0.3338 - val_loss: 6.5732e-04 - val_mse: 6.5732e-04 - val_logcosh: 3.2828e-04 - val_acc: 0.3293\n",
      "Epoch 96/100\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 1.2151e-04 - mse: 1.2151e-04 - logcosh: 6.0748e-05 - acc: 0.3338 - val_loss: 6.3786e-04 - val_mse: 6.3786e-04 - val_logcosh: 3.1857e-04 - val_acc: 0.3293\n",
      "Epoch 97/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.1906e-04 - mse: 1.1906e-04 - logcosh: 5.9526e-05 - acc: 0.3338 - val_loss: 6.2568e-04 - val_mse: 6.2568e-04 - val_logcosh: 3.1251e-04 - val_acc: 0.3293\n",
      "Epoch 98/100\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 1.2077e-04 - mse: 1.2077e-04 - logcosh: 6.0375e-05 - acc: 0.3338 - val_loss: 6.5664e-04 - val_mse: 6.5664e-04 - val_logcosh: 3.2796e-04 - val_acc: 0.3293\n",
      "Epoch 99/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.1873e-04 - mse: 1.1873e-04 - logcosh: 5.9356e-05 - acc: 0.3338 - val_loss: 6.2272e-04 - val_mse: 6.2272e-04 - val_logcosh: 3.1100e-04 - val_acc: 0.3293\n",
      "Epoch 100/100\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 1.1725e-04 - mse: 1.1725e-04 - logcosh: 5.8621e-05 - acc: 0.3338 - val_loss: 6.2717e-04 - val_mse: 6.2717e-04 - val_logcosh: 3.1324e-04 - val_acc: 0.3293\n"
     ]
    }
   ],
   "source": [
    "model = compiled_tcn(return_sequences=False,\n",
    "                     regression=True,\n",
    "                     num_feat=3,\n",
    "                     num_classes=1,\n",
    "\n",
    "                     kernel_size=8,\n",
    "                     padding='causal',\n",
    "                     dilations=[2 ** i for i in range(2)],\n",
    "                     nb_stacks=1,\n",
    "                     learning_rate=1e-4,\n",
    "                     max_len=None,\n",
    "                     use_weight_norm=True,\n",
    "                     use_skip_connections=True,\n",
    "\n",
    "\n",
    "                     nb_filters=128)\n",
    "\n",
    "# model.summary()\n",
    "history = model.fit(x=datas_list_wear, y=wear_label, batch_size=32, epochs=100, validation_split=0.1, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 5.3224e-05 - mse: 5.3224e-05 - logcosh: 2.6611e-05 - acc: 0.3338 - val_loss: 3.6242e-04 - val_mse: 3.6242e-04 - val_logcosh: 1.8100e-04 - val_acc: 0.3293\n",
      "Epoch 2/200\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 5.1011e-05 - mse: 5.1011e-05 - logcosh: 2.5506e-05 - acc: 0.3338 - val_loss: 3.5915e-04 - val_mse: 3.5915e-04 - val_logcosh: 1.7935e-04 - val_acc: 0.3293\n",
      "Epoch 3/200\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 5.3221e-05 - mse: 5.3221e-05 - logcosh: 2.6611e-05 - acc: 0.3338 - val_loss: 3.8030e-04 - val_mse: 3.8030e-04 - val_logcosh: 1.8991e-04 - val_acc: 0.3293\n",
      "Epoch 4/200\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 5.0628e-05 - mse: 5.0628e-05 - logcosh: 2.5315e-05 - acc: 0.3338 - val_loss: 3.4768e-04 - val_mse: 3.4768e-04 - val_logcosh: 1.7365e-04 - val_acc: 0.3293\n",
      "Epoch 5/200\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 4.8460e-05 - mse: 4.8460e-05 - logcosh: 2.4231e-05 - acc: 0.3338 - val_loss: 3.5659e-04 - val_mse: 3.5659e-04 - val_logcosh: 1.7808e-04 - val_acc: 0.3293\n",
      "Epoch 6/200\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 5.0801e-05 - mse: 5.0801e-05 - logcosh: 2.5401e-05 - acc: 0.3338 - val_loss: 3.7034e-04 - val_mse: 3.7034e-04 - val_logcosh: 1.8494e-04 - val_acc: 0.3293\n",
      "Epoch 7/200\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 4.9919e-05 - mse: 4.9919e-05 - logcosh: 2.4960e-05 - acc: 0.3338 - val_loss: 3.6156e-04 - val_mse: 3.6156e-04 - val_logcosh: 1.8054e-04 - val_acc: 0.3293\n",
      "Epoch 8/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.8841e-05 - mse: 4.8841e-05 - logcosh: 2.4422e-05 - acc: 0.3338 - val_loss: 3.5555e-04 - val_mse: 3.5555e-04 - val_logcosh: 1.7756e-04 - val_acc: 0.3293\n",
      "Epoch 9/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 4.8070e-05 - mse: 4.8070e-05 - logcosh: 2.4036e-05 - acc: 0.3338 - val_loss: 3.5203e-04 - val_mse: 3.5203e-04 - val_logcosh: 1.7580e-04 - val_acc: 0.3293\n",
      "Epoch 10/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.6336e-05 - mse: 4.6336e-05 - logcosh: 2.3170e-05 - acc: 0.3338 - val_loss: 3.4308e-04 - val_mse: 3.4308e-04 - val_logcosh: 1.7134e-04 - val_acc: 0.3293\n",
      "Epoch 11/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.8939e-05 - mse: 4.8939e-05 - logcosh: 2.4470e-05 - acc: 0.3338 - val_loss: 3.4652e-04 - val_mse: 3.4652e-04 - val_logcosh: 1.7305e-04 - val_acc: 0.3293\n",
      "Epoch 12/200\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 4.8407e-05 - mse: 4.8407e-05 - logcosh: 2.4205e-05 - acc: 0.3338 - val_loss: 3.6773e-04 - val_mse: 3.6773e-04 - val_logcosh: 1.8363e-04 - val_acc: 0.3293\n",
      "Epoch 13/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.9275e-05 - mse: 4.9275e-05 - logcosh: 2.4637e-05 - acc: 0.3338 - val_loss: 3.6161e-04 - val_mse: 3.6161e-04 - val_logcosh: 1.8058e-04 - val_acc: 0.3293\n",
      "Epoch 14/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.8650e-05 - mse: 4.8650e-05 - logcosh: 2.4327e-05 - acc: 0.3338 - val_loss: 3.4885e-04 - val_mse: 3.4885e-04 - val_logcosh: 1.7421e-04 - val_acc: 0.3293\n",
      "Epoch 15/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.9853e-05 - mse: 4.9853e-05 - logcosh: 2.4927e-05 - acc: 0.3338 - val_loss: 3.4593e-04 - val_mse: 3.4593e-04 - val_logcosh: 1.7276e-04 - val_acc: 0.3293\n",
      "Epoch 16/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.9396e-05 - mse: 4.9396e-05 - logcosh: 2.4699e-05 - acc: 0.3338 - val_loss: 3.5097e-04 - val_mse: 3.5097e-04 - val_logcosh: 1.7526e-04 - val_acc: 0.3293\n",
      "Epoch 17/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 5.0960e-05 - mse: 5.0960e-05 - logcosh: 2.5479e-05 - acc: 0.3338 - val_loss: 3.4422e-04 - val_mse: 3.4422e-04 - val_logcosh: 1.7189e-04 - val_acc: 0.3293\n",
      "Epoch 18/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.5855e-05 - mse: 4.5855e-05 - logcosh: 2.2928e-05 - acc: 0.3338 - val_loss: 3.3834e-04 - val_mse: 3.3834e-04 - val_logcosh: 1.6896e-04 - val_acc: 0.3293\n",
      "Epoch 19/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.7215e-05 - mse: 4.7215e-05 - logcosh: 2.3608e-05 - acc: 0.3338 - val_loss: 3.1755e-04 - val_mse: 3.1755e-04 - val_logcosh: 1.5859e-04 - val_acc: 0.3293\n",
      "Epoch 20/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.6956e-05 - mse: 4.6956e-05 - logcosh: 2.3478e-05 - acc: 0.3338 - val_loss: 3.3938e-04 - val_mse: 3.3938e-04 - val_logcosh: 1.6948e-04 - val_acc: 0.3293\n",
      "Epoch 21/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.4904e-05 - mse: 4.4904e-05 - logcosh: 2.2453e-05 - acc: 0.3338 - val_loss: 3.2941e-04 - val_mse: 3.2941e-04 - val_logcosh: 1.6450e-04 - val_acc: 0.3293\n",
      "Epoch 22/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.8035e-05 - mse: 4.8035e-05 - logcosh: 2.4018e-05 - acc: 0.3338 - val_loss: 3.3786e-04 - val_mse: 3.3786e-04 - val_logcosh: 1.6873e-04 - val_acc: 0.3293\n",
      "Epoch 23/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.6573e-05 - mse: 4.6573e-05 - logcosh: 2.3287e-05 - acc: 0.3338 - val_loss: 3.2085e-04 - val_mse: 3.2085e-04 - val_logcosh: 1.6025e-04 - val_acc: 0.3293\n",
      "Epoch 24/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.4680e-05 - mse: 4.4680e-05 - logcosh: 2.2342e-05 - acc: 0.3338 - val_loss: 3.2554e-04 - val_mse: 3.2554e-04 - val_logcosh: 1.6257e-04 - val_acc: 0.3293\n",
      "Epoch 25/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.2503e-05 - mse: 4.2503e-05 - logcosh: 2.1253e-05 - acc: 0.3338 - val_loss: 3.4042e-04 - val_mse: 3.4042e-04 - val_logcosh: 1.6999e-04 - val_acc: 0.3293\n",
      "Epoch 26/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 4.7319e-05 - mse: 4.7319e-05 - logcosh: 2.3661e-05 - acc: 0.3338 - val_loss: 3.1864e-04 - val_mse: 3.1864e-04 - val_logcosh: 1.5913e-04 - val_acc: 0.3293\n",
      "Epoch 27/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 4.6482e-05 - mse: 4.6482e-05 - logcosh: 2.3242e-05 - acc: 0.3338 - val_loss: 3.4176e-04 - val_mse: 3.4176e-04 - val_logcosh: 1.7067e-04 - val_acc: 0.3293\n",
      "Epoch 28/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 4.2597e-05 - mse: 4.2597e-05 - logcosh: 2.1300e-05 - acc: 0.3338 - val_loss: 3.2294e-04 - val_mse: 3.2294e-04 - val_logcosh: 1.6127e-04 - val_acc: 0.3293\n",
      "Epoch 29/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 4.5679e-05 - mse: 4.5679e-05 - logcosh: 2.2839e-05 - acc: 0.3338 - val_loss: 3.3229e-04 - val_mse: 3.3229e-04 - val_logcosh: 1.6592e-04 - val_acc: 0.3293\n",
      "Epoch 30/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.6117e-05 - mse: 4.6117e-05 - logcosh: 2.3060e-05 - acc: 0.3338 - val_loss: 3.0435e-04 - val_mse: 3.0435e-04 - val_logcosh: 1.5200e-04 - val_acc: 0.3293\n",
      "Epoch 31/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.4305e-05 - mse: 4.4305e-05 - logcosh: 2.2154e-05 - acc: 0.3338 - val_loss: 3.1109e-04 - val_mse: 3.1109e-04 - val_logcosh: 1.5536e-04 - val_acc: 0.3293\n",
      "Epoch 32/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.9794e-05 - mse: 3.9794e-05 - logcosh: 1.9899e-05 - acc: 0.3338 - val_loss: 3.0643e-04 - val_mse: 3.0643e-04 - val_logcosh: 1.5303e-04 - val_acc: 0.3293\n",
      "Epoch 33/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.3822e-05 - mse: 4.3822e-05 - logcosh: 2.1912e-05 - acc: 0.3338 - val_loss: 3.0011e-04 - val_mse: 3.0011e-04 - val_logcosh: 1.4988e-04 - val_acc: 0.3293\n",
      "Epoch 34/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.2190e-05 - mse: 4.2190e-05 - logcosh: 2.1097e-05 - acc: 0.3338 - val_loss: 3.0109e-04 - val_mse: 3.0109e-04 - val_logcosh: 1.5037e-04 - val_acc: 0.3293\n",
      "Epoch 35/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.3563e-05 - mse: 4.3563e-05 - logcosh: 2.1783e-05 - acc: 0.3338 - val_loss: 3.0375e-04 - val_mse: 3.0375e-04 - val_logcosh: 1.5170e-04 - val_acc: 0.3293\n",
      "Epoch 36/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.3564e-05 - mse: 4.3564e-05 - logcosh: 2.1783e-05 - acc: 0.3338 - val_loss: 3.3660e-04 - val_mse: 3.3660e-04 - val_logcosh: 1.6808e-04 - val_acc: 0.3293\n",
      "Epoch 37/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.2215e-05 - mse: 4.2215e-05 - logcosh: 2.1109e-05 - acc: 0.3338 - val_loss: 3.0542e-04 - val_mse: 3.0542e-04 - val_logcosh: 1.5253e-04 - val_acc: 0.3293\n",
      "Epoch 38/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.1899e-05 - mse: 4.1899e-05 - logcosh: 2.0951e-05 - acc: 0.3338 - val_loss: 3.1199e-04 - val_mse: 3.1199e-04 - val_logcosh: 1.5580e-04 - val_acc: 0.3293\n",
      "Epoch 39/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 4.0356e-05 - mse: 4.0356e-05 - logcosh: 2.0179e-05 - acc: 0.3338 - val_loss: 2.9375e-04 - val_mse: 2.9375e-04 - val_logcosh: 1.4670e-04 - val_acc: 0.3293\n",
      "Epoch 40/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.2710e-05 - mse: 4.2710e-05 - logcosh: 2.1357e-05 - acc: 0.3338 - val_loss: 3.1495e-04 - val_mse: 3.1495e-04 - val_logcosh: 1.5728e-04 - val_acc: 0.3293\n",
      "Epoch 41/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 3.9467e-05 - mse: 3.9467e-05 - logcosh: 1.9736e-05 - acc: 0.3338 - val_loss: 3.1010e-04 - val_mse: 3.1010e-04 - val_logcosh: 1.5485e-04 - val_acc: 0.3293\n",
      "Epoch 42/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 4.4000e-05 - mse: 4.4000e-05 - logcosh: 2.2001e-05 - acc: 0.3338 - val_loss: 3.1096e-04 - val_mse: 3.1096e-04 - val_logcosh: 1.5528e-04 - val_acc: 0.3293\n",
      "Epoch 43/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.4838e-05 - mse: 4.4838e-05 - logcosh: 2.2420e-05 - acc: 0.3338 - val_loss: 2.9592e-04 - val_mse: 2.9592e-04 - val_logcosh: 1.4778e-04 - val_acc: 0.3293\n",
      "Epoch 44/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 3.9487e-05 - mse: 3.9487e-05 - logcosh: 1.9745e-05 - acc: 0.3338 - val_loss: 2.9851e-04 - val_mse: 2.9851e-04 - val_logcosh: 1.4905e-04 - val_acc: 0.3293\n",
      "Epoch 45/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.9025e-05 - mse: 3.9025e-05 - logcosh: 1.9515e-05 - acc: 0.3338 - val_loss: 2.9818e-04 - val_mse: 2.9818e-04 - val_logcosh: 1.4891e-04 - val_acc: 0.3293\n",
      "Epoch 46/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.3607e-05 - mse: 4.3607e-05 - logcosh: 2.1805e-05 - acc: 0.3338 - val_loss: 3.0676e-04 - val_mse: 3.0676e-04 - val_logcosh: 1.5320e-04 - val_acc: 0.3293\n",
      "Epoch 47/200\n",
      "422/422 [==============================] - 37s 87ms/step - loss: 4.1149e-05 - mse: 4.1149e-05 - logcosh: 2.0576e-05 - acc: 0.3338 - val_loss: 2.8931e-04 - val_mse: 2.8931e-04 - val_logcosh: 1.4448e-04 - val_acc: 0.3293\n",
      "Epoch 48/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.0727e-05 - mse: 4.0727e-05 - logcosh: 2.0364e-05 - acc: 0.3338 - val_loss: 2.9071e-04 - val_mse: 2.9071e-04 - val_logcosh: 1.4517e-04 - val_acc: 0.3293\n",
      "Epoch 49/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.9895e-05 - mse: 3.9895e-05 - logcosh: 1.9949e-05 - acc: 0.3338 - val_loss: 3.1085e-04 - val_mse: 3.1085e-04 - val_logcosh: 1.5523e-04 - val_acc: 0.3293\n",
      "Epoch 50/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.1184e-05 - mse: 4.1184e-05 - logcosh: 2.0593e-05 - acc: 0.3338 - val_loss: 2.9579e-04 - val_mse: 2.9579e-04 - val_logcosh: 1.4771e-04 - val_acc: 0.3293\n",
      "Epoch 51/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 4.1618e-05 - mse: 4.1618e-05 - logcosh: 2.0811e-05 - acc: 0.3338 - val_loss: 3.2489e-04 - val_mse: 3.2489e-04 - val_logcosh: 1.6224e-04 - val_acc: 0.3293\n",
      "Epoch 52/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.7896e-05 - mse: 3.7896e-05 - logcosh: 1.8950e-05 - acc: 0.3338 - val_loss: 2.9304e-04 - val_mse: 2.9304e-04 - val_logcosh: 1.4633e-04 - val_acc: 0.3293\n",
      "Epoch 53/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.8776e-05 - mse: 3.8776e-05 - logcosh: 1.9390e-05 - acc: 0.3338 - val_loss: 2.7478e-04 - val_mse: 2.7478e-04 - val_logcosh: 1.3722e-04 - val_acc: 0.3293\n",
      "Epoch 54/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.8102e-05 - mse: 3.8102e-05 - logcosh: 1.9053e-05 - acc: 0.3338 - val_loss: 3.1052e-04 - val_mse: 3.1052e-04 - val_logcosh: 1.5506e-04 - val_acc: 0.3293\n",
      "Epoch 55/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 4.0440e-05 - mse: 4.0440e-05 - logcosh: 2.0221e-05 - acc: 0.3338 - val_loss: 2.8705e-04 - val_mse: 2.8705e-04 - val_logcosh: 1.4333e-04 - val_acc: 0.3293\n",
      "Epoch 56/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.8770e-05 - mse: 3.8770e-05 - logcosh: 1.9387e-05 - acc: 0.3338 - val_loss: 2.9609e-04 - val_mse: 2.9609e-04 - val_logcosh: 1.4786e-04 - val_acc: 0.3293\n",
      "Epoch 57/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 4.0919e-05 - mse: 4.0919e-05 - logcosh: 2.0460e-05 - acc: 0.3338 - val_loss: 2.8877e-04 - val_mse: 2.8877e-04 - val_logcosh: 1.4420e-04 - val_acc: 0.3293\n",
      "Epoch 58/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 4.1218e-05 - mse: 4.1218e-05 - logcosh: 2.0610e-05 - acc: 0.3338 - val_loss: 2.7374e-04 - val_mse: 2.7374e-04 - val_logcosh: 1.3670e-04 - val_acc: 0.3293\n",
      "Epoch 59/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.7908e-05 - mse: 3.7908e-05 - logcosh: 1.8956e-05 - acc: 0.3338 - val_loss: 2.9831e-04 - val_mse: 2.9831e-04 - val_logcosh: 1.4897e-04 - val_acc: 0.3293\n",
      "Epoch 60/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.7710e-05 - mse: 3.7710e-05 - logcosh: 1.8856e-05 - acc: 0.3338 - val_loss: 2.9466e-04 - val_mse: 2.9466e-04 - val_logcosh: 1.4716e-04 - val_acc: 0.3293\n",
      "Epoch 61/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.6982e-05 - mse: 3.6982e-05 - logcosh: 1.8493e-05 - acc: 0.3338 - val_loss: 2.8532e-04 - val_mse: 2.8532e-04 - val_logcosh: 1.4248e-04 - val_acc: 0.3293\n",
      "Epoch 62/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.7104e-05 - mse: 3.7104e-05 - logcosh: 1.8554e-05 - acc: 0.3338 - val_loss: 2.7219e-04 - val_mse: 2.7219e-04 - val_logcosh: 1.3592e-04 - val_acc: 0.3293\n",
      "Epoch 63/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.7644e-05 - mse: 3.7644e-05 - logcosh: 1.8824e-05 - acc: 0.3338 - val_loss: 2.8297e-04 - val_mse: 2.8297e-04 - val_logcosh: 1.4131e-04 - val_acc: 0.3293\n",
      "Epoch 64/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.8422e-05 - mse: 3.8422e-05 - logcosh: 1.9212e-05 - acc: 0.3338 - val_loss: 3.0969e-04 - val_mse: 3.0969e-04 - val_logcosh: 1.5464e-04 - val_acc: 0.3293\n",
      "Epoch 65/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.8442e-05 - mse: 3.8442e-05 - logcosh: 1.9223e-05 - acc: 0.3338 - val_loss: 2.9333e-04 - val_mse: 2.9333e-04 - val_logcosh: 1.4649e-04 - val_acc: 0.3293\n",
      "Epoch 66/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.7364e-05 - mse: 3.7364e-05 - logcosh: 1.8684e-05 - acc: 0.3338 - val_loss: 2.8724e-04 - val_mse: 2.8724e-04 - val_logcosh: 1.4344e-04 - val_acc: 0.3293\n",
      "Epoch 67/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.9421e-05 - mse: 3.9421e-05 - logcosh: 1.9711e-05 - acc: 0.3338 - val_loss: 3.0714e-04 - val_mse: 3.0714e-04 - val_logcosh: 1.5336e-04 - val_acc: 0.3293\n",
      "Epoch 68/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.8729e-05 - mse: 3.8729e-05 - logcosh: 1.9366e-05 - acc: 0.3338 - val_loss: 2.8826e-04 - val_mse: 2.8826e-04 - val_logcosh: 1.4396e-04 - val_acc: 0.3293\n",
      "Epoch 69/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.7757e-05 - mse: 3.7757e-05 - logcosh: 1.8880e-05 - acc: 0.3338 - val_loss: 2.6524e-04 - val_mse: 2.6524e-04 - val_logcosh: 1.3247e-04 - val_acc: 0.3293\n",
      "Epoch 70/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 4.0572e-05 - mse: 4.0572e-05 - logcosh: 2.0288e-05 - acc: 0.3338 - val_loss: 2.7322e-04 - val_mse: 2.7322e-04 - val_logcosh: 1.3646e-04 - val_acc: 0.3293\n",
      "Epoch 71/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.6131e-05 - mse: 3.6131e-05 - logcosh: 1.8067e-05 - acc: 0.3338 - val_loss: 2.7646e-04 - val_mse: 2.7646e-04 - val_logcosh: 1.3806e-04 - val_acc: 0.3293\n",
      "Epoch 72/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.7229e-05 - mse: 3.7229e-05 - logcosh: 1.8616e-05 - acc: 0.3338 - val_loss: 2.5793e-04 - val_mse: 2.5793e-04 - val_logcosh: 1.2882e-04 - val_acc: 0.3293\n",
      "Epoch 73/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.5475e-05 - mse: 3.5475e-05 - logcosh: 1.7739e-05 - acc: 0.3338 - val_loss: 2.5337e-04 - val_mse: 2.5337e-04 - val_logcosh: 1.2653e-04 - val_acc: 0.3293\n",
      "Epoch 74/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.7033e-05 - mse: 3.7033e-05 - logcosh: 1.8519e-05 - acc: 0.3338 - val_loss: 3.0673e-04 - val_mse: 3.0673e-04 - val_logcosh: 1.5318e-04 - val_acc: 0.3293\n",
      "Epoch 75/200\n",
      "422/422 [==============================] - 39s 93ms/step - loss: 3.9781e-05 - mse: 3.9781e-05 - logcosh: 1.9890e-05 - acc: 0.3338 - val_loss: 2.7109e-04 - val_mse: 2.7109e-04 - val_logcosh: 1.3539e-04 - val_acc: 0.3293\n",
      "Epoch 76/200\n",
      "422/422 [==============================] - 39s 93ms/step - loss: 3.9667e-05 - mse: 3.9667e-05 - logcosh: 1.9833e-05 - acc: 0.3338 - val_loss: 2.8037e-04 - val_mse: 2.8037e-04 - val_logcosh: 1.4001e-04 - val_acc: 0.3293\n",
      "Epoch 77/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 3.5086e-05 - mse: 3.5086e-05 - logcosh: 1.7545e-05 - acc: 0.3338 - val_loss: 2.7282e-04 - val_mse: 2.7282e-04 - val_logcosh: 1.3624e-04 - val_acc: 0.3293\n",
      "Epoch 78/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.5301e-05 - mse: 3.5301e-05 - logcosh: 1.7653e-05 - acc: 0.3338 - val_loss: 2.4683e-04 - val_mse: 2.4683e-04 - val_logcosh: 1.2327e-04 - val_acc: 0.3293\n",
      "Epoch 79/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.5825e-05 - mse: 3.5825e-05 - logcosh: 1.7915e-05 - acc: 0.3338 - val_loss: 2.6868e-04 - val_mse: 2.6868e-04 - val_logcosh: 1.3418e-04 - val_acc: 0.3293\n",
      "Epoch 80/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.6575e-05 - mse: 3.6575e-05 - logcosh: 1.8289e-05 - acc: 0.3338 - val_loss: 2.5966e-04 - val_mse: 2.5966e-04 - val_logcosh: 1.2967e-04 - val_acc: 0.3293\n",
      "Epoch 81/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.8017e-05 - mse: 3.8017e-05 - logcosh: 1.9010e-05 - acc: 0.3338 - val_loss: 2.8628e-04 - val_mse: 2.8628e-04 - val_logcosh: 1.4295e-04 - val_acc: 0.3293\n",
      "Epoch 82/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.6150e-05 - mse: 3.6150e-05 - logcosh: 1.8077e-05 - acc: 0.3338 - val_loss: 2.6518e-04 - val_mse: 2.6518e-04 - val_logcosh: 1.3243e-04 - val_acc: 0.3293\n",
      "Epoch 83/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.5708e-05 - mse: 3.5708e-05 - logcosh: 1.7855e-05 - acc: 0.3338 - val_loss: 2.7498e-04 - val_mse: 2.7498e-04 - val_logcosh: 1.3731e-04 - val_acc: 0.3293\n",
      "Epoch 84/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.5724e-05 - mse: 3.5724e-05 - logcosh: 1.7863e-05 - acc: 0.3338 - val_loss: 2.5047e-04 - val_mse: 2.5047e-04 - val_logcosh: 1.2508e-04 - val_acc: 0.3293\n",
      "Epoch 85/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.6767e-05 - mse: 3.6767e-05 - logcosh: 1.8386e-05 - acc: 0.3338 - val_loss: 2.6175e-04 - val_mse: 2.6175e-04 - val_logcosh: 1.3073e-04 - val_acc: 0.3293\n",
      "Epoch 86/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.4981e-05 - mse: 3.4981e-05 - logcosh: 1.7493e-05 - acc: 0.3338 - val_loss: 2.6292e-04 - val_mse: 2.6292e-04 - val_logcosh: 1.3128e-04 - val_acc: 0.3293\n",
      "Epoch 87/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.5207e-05 - mse: 3.5207e-05 - logcosh: 1.7605e-05 - acc: 0.3338 - val_loss: 2.6974e-04 - val_mse: 2.6974e-04 - val_logcosh: 1.3470e-04 - val_acc: 0.3293\n",
      "Epoch 88/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.4112e-05 - mse: 3.4112e-05 - logcosh: 1.7059e-05 - acc: 0.3338 - val_loss: 2.5332e-04 - val_mse: 2.5332e-04 - val_logcosh: 1.2651e-04 - val_acc: 0.3293\n",
      "Epoch 89/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.3879e-05 - mse: 3.3879e-05 - logcosh: 1.6942e-05 - acc: 0.3338 - val_loss: 2.5314e-04 - val_mse: 2.5314e-04 - val_logcosh: 1.2644e-04 - val_acc: 0.3293\n",
      "Epoch 90/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.6421e-05 - mse: 3.6421e-05 - logcosh: 1.8212e-05 - acc: 0.3338 - val_loss: 2.5132e-04 - val_mse: 2.5132e-04 - val_logcosh: 1.2551e-04 - val_acc: 0.3293\n",
      "Epoch 91/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.4068e-05 - mse: 3.4068e-05 - logcosh: 1.7036e-05 - acc: 0.3338 - val_loss: 2.4180e-04 - val_mse: 2.4180e-04 - val_logcosh: 1.2075e-04 - val_acc: 0.3293\n",
      "Epoch 92/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.5492e-05 - mse: 3.5492e-05 - logcosh: 1.7747e-05 - acc: 0.3338 - val_loss: 2.4143e-04 - val_mse: 2.4143e-04 - val_logcosh: 1.2057e-04 - val_acc: 0.3293\n",
      "Epoch 93/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.2515e-05 - mse: 3.2515e-05 - logcosh: 1.6260e-05 - acc: 0.3338 - val_loss: 2.3437e-04 - val_mse: 2.3437e-04 - val_logcosh: 1.1704e-04 - val_acc: 0.3293\n",
      "Epoch 94/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.3425e-05 - mse: 3.3425e-05 - logcosh: 1.6715e-05 - acc: 0.3338 - val_loss: 2.4704e-04 - val_mse: 2.4704e-04 - val_logcosh: 1.2337e-04 - val_acc: 0.3293\n",
      "Epoch 95/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.3756e-05 - mse: 3.3756e-05 - logcosh: 1.6880e-05 - acc: 0.3338 - val_loss: 2.4556e-04 - val_mse: 2.4556e-04 - val_logcosh: 1.2263e-04 - val_acc: 0.3293\n",
      "Epoch 96/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.5252e-05 - mse: 3.5252e-05 - logcosh: 1.7626e-05 - acc: 0.3338 - val_loss: 2.5890e-04 - val_mse: 2.5890e-04 - val_logcosh: 1.2930e-04 - val_acc: 0.3293\n",
      "Epoch 97/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.2872e-05 - mse: 3.2872e-05 - logcosh: 1.6438e-05 - acc: 0.3338 - val_loss: 2.2690e-04 - val_mse: 2.2690e-04 - val_logcosh: 1.1331e-04 - val_acc: 0.3293\n",
      "Epoch 98/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.0935e-05 - mse: 3.0935e-05 - logcosh: 1.5470e-05 - acc: 0.3338 - val_loss: 2.5227e-04 - val_mse: 2.5227e-04 - val_logcosh: 1.2597e-04 - val_acc: 0.3293\n",
      "Epoch 99/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.4822e-05 - mse: 3.4822e-05 - logcosh: 1.7413e-05 - acc: 0.3338 - val_loss: 2.4038e-04 - val_mse: 2.4038e-04 - val_logcosh: 1.2004e-04 - val_acc: 0.3293\n",
      "Epoch 100/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.2831e-05 - mse: 3.2831e-05 - logcosh: 1.6418e-05 - acc: 0.3338 - val_loss: 2.4972e-04 - val_mse: 2.4972e-04 - val_logcosh: 1.2470e-04 - val_acc: 0.3293\n",
      "Epoch 101/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.2469e-05 - mse: 3.2469e-05 - logcosh: 1.6237e-05 - acc: 0.3338 - val_loss: 2.2964e-04 - val_mse: 2.2964e-04 - val_logcosh: 1.1468e-04 - val_acc: 0.3293\n",
      "Epoch 102/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.4728e-05 - mse: 3.4728e-05 - logcosh: 1.7365e-05 - acc: 0.3338 - val_loss: 2.5841e-04 - val_mse: 2.5841e-04 - val_logcosh: 1.2904e-04 - val_acc: 0.3293\n",
      "Epoch 103/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.3496e-05 - mse: 3.3496e-05 - logcosh: 1.6749e-05 - acc: 0.3338 - val_loss: 2.9271e-04 - val_mse: 2.9271e-04 - val_logcosh: 1.4614e-04 - val_acc: 0.3293\n",
      "Epoch 104/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.4048e-05 - mse: 3.4048e-05 - logcosh: 1.7027e-05 - acc: 0.3338 - val_loss: 2.4458e-04 - val_mse: 2.4458e-04 - val_logcosh: 1.2213e-04 - val_acc: 0.3293\n",
      "Epoch 105/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.3653e-05 - mse: 3.3653e-05 - logcosh: 1.6827e-05 - acc: 0.3338 - val_loss: 2.6478e-04 - val_mse: 2.6478e-04 - val_logcosh: 1.3221e-04 - val_acc: 0.3293\n",
      "Epoch 106/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.0701e-05 - mse: 3.0701e-05 - logcosh: 1.5353e-05 - acc: 0.3338 - val_loss: 2.3112e-04 - val_mse: 2.3112e-04 - val_logcosh: 1.1542e-04 - val_acc: 0.3293\n",
      "Epoch 107/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.0771e-05 - mse: 3.0771e-05 - logcosh: 1.5388e-05 - acc: 0.3338 - val_loss: 2.2987e-04 - val_mse: 2.2987e-04 - val_logcosh: 1.1479e-04 - val_acc: 0.3293\n",
      "Epoch 108/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.0093e-05 - mse: 3.0093e-05 - logcosh: 1.5049e-05 - acc: 0.3338 - val_loss: 2.3078e-04 - val_mse: 2.3078e-04 - val_logcosh: 1.1526e-04 - val_acc: 0.3293\n",
      "Epoch 109/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.3482e-05 - mse: 3.3482e-05 - logcosh: 1.6743e-05 - acc: 0.3338 - val_loss: 2.6280e-04 - val_mse: 2.6280e-04 - val_logcosh: 1.3124e-04 - val_acc: 0.3293\n",
      "Epoch 110/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.2342e-05 - mse: 3.2342e-05 - logcosh: 1.6174e-05 - acc: 0.3338 - val_loss: 2.2678e-04 - val_mse: 2.2678e-04 - val_logcosh: 1.1325e-04 - val_acc: 0.3293\n",
      "Epoch 111/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.2386e-05 - mse: 3.2386e-05 - logcosh: 1.6195e-05 - acc: 0.3338 - val_loss: 2.5217e-04 - val_mse: 2.5217e-04 - val_logcosh: 1.2591e-04 - val_acc: 0.3293\n",
      "Epoch 112/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.3585e-05 - mse: 3.3585e-05 - logcosh: 1.6794e-05 - acc: 0.3338 - val_loss: 2.5142e-04 - val_mse: 2.5142e-04 - val_logcosh: 1.2557e-04 - val_acc: 0.3293\n",
      "Epoch 113/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.2223e-05 - mse: 3.2223e-05 - logcosh: 1.6114e-05 - acc: 0.3338 - val_loss: 2.4748e-04 - val_mse: 2.4748e-04 - val_logcosh: 1.2358e-04 - val_acc: 0.3293\n",
      "Epoch 114/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.3737e-05 - mse: 3.3737e-05 - logcosh: 1.6869e-05 - acc: 0.3338 - val_loss: 2.3250e-04 - val_mse: 2.3250e-04 - val_logcosh: 1.1611e-04 - val_acc: 0.3293\n",
      "Epoch 115/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.0959e-05 - mse: 3.0959e-05 - logcosh: 1.5482e-05 - acc: 0.3338 - val_loss: 2.2989e-04 - val_mse: 2.2989e-04 - val_logcosh: 1.1481e-04 - val_acc: 0.3293\n",
      "Epoch 116/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.3945e-05 - mse: 3.3945e-05 - logcosh: 1.6972e-05 - acc: 0.3338 - val_loss: 2.2835e-04 - val_mse: 2.2835e-04 - val_logcosh: 1.1404e-04 - val_acc: 0.3293\n",
      "Epoch 117/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.9604e-05 - mse: 2.9604e-05 - logcosh: 1.4805e-05 - acc: 0.3338 - val_loss: 2.4388e-04 - val_mse: 2.4388e-04 - val_logcosh: 1.2179e-04 - val_acc: 0.3293\n",
      "Epoch 118/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.0604e-05 - mse: 3.0604e-05 - logcosh: 1.5303e-05 - acc: 0.3338 - val_loss: 2.2376e-04 - val_mse: 2.2376e-04 - val_logcosh: 1.1175e-04 - val_acc: 0.3293\n",
      "Epoch 119/200\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 3.2295e-05 - mse: 3.2295e-05 - logcosh: 1.6150e-05 - acc: 0.3338 - val_loss: 2.4596e-04 - val_mse: 2.4596e-04 - val_logcosh: 1.2281e-04 - val_acc: 0.3293\n",
      "Epoch 120/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.5907e-05 - mse: 3.5907e-05 - logcosh: 1.7955e-05 - acc: 0.3338 - val_loss: 2.3238e-04 - val_mse: 2.3238e-04 - val_logcosh: 1.1604e-04 - val_acc: 0.3293\n",
      "Epoch 121/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.2486e-05 - mse: 3.2486e-05 - logcosh: 1.6245e-05 - acc: 0.3338 - val_loss: 2.2542e-04 - val_mse: 2.2542e-04 - val_logcosh: 1.1258e-04 - val_acc: 0.3293\n",
      "Epoch 122/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 2.9734e-05 - mse: 2.9734e-05 - logcosh: 1.4870e-05 - acc: 0.3338 - val_loss: 2.3452e-04 - val_mse: 2.3452e-04 - val_logcosh: 1.1711e-04 - val_acc: 0.3293\n",
      "Epoch 123/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 3.2883e-05 - mse: 3.2883e-05 - logcosh: 1.6443e-05 - acc: 0.3338 - val_loss: 2.2884e-04 - val_mse: 2.2884e-04 - val_logcosh: 1.1429e-04 - val_acc: 0.3293\n",
      "Epoch 124/200\n",
      "422/422 [==============================] - 39s 93ms/step - loss: 3.1647e-05 - mse: 3.1647e-05 - logcosh: 1.5826e-05 - acc: 0.3338 - val_loss: 2.3699e-04 - val_mse: 2.3699e-04 - val_logcosh: 1.1834e-04 - val_acc: 0.3293\n",
      "Epoch 125/200\n",
      "422/422 [==============================] - 40s 94ms/step - loss: 2.9888e-05 - mse: 2.9888e-05 - logcosh: 1.4946e-05 - acc: 0.3338 - val_loss: 2.4935e-04 - val_mse: 2.4935e-04 - val_logcosh: 1.2452e-04 - val_acc: 0.3293\n",
      "Epoch 126/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.0752e-05 - mse: 3.0752e-05 - logcosh: 1.5379e-05 - acc: 0.3338 - val_loss: 2.3289e-04 - val_mse: 2.3289e-04 - val_logcosh: 1.1629e-04 - val_acc: 0.3293\n",
      "Epoch 127/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.1841e-05 - mse: 3.1841e-05 - logcosh: 1.5923e-05 - acc: 0.3338 - val_loss: 2.3826e-04 - val_mse: 2.3826e-04 - val_logcosh: 1.1897e-04 - val_acc: 0.3293\n",
      "Epoch 128/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.1136e-05 - mse: 3.1136e-05 - logcosh: 1.5570e-05 - acc: 0.3338 - val_loss: 2.6518e-04 - val_mse: 2.6518e-04 - val_logcosh: 1.3241e-04 - val_acc: 0.3293\n",
      "Epoch 129/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.1562e-05 - mse: 3.1562e-05 - logcosh: 1.5783e-05 - acc: 0.3338 - val_loss: 2.5532e-04 - val_mse: 2.5532e-04 - val_logcosh: 1.2749e-04 - val_acc: 0.3293\n",
      "Epoch 130/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.0383e-05 - mse: 3.0383e-05 - logcosh: 1.5194e-05 - acc: 0.3338 - val_loss: 2.1748e-04 - val_mse: 2.1748e-04 - val_logcosh: 1.0860e-04 - val_acc: 0.3293\n",
      "Epoch 131/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 3.1030e-05 - mse: 3.1030e-05 - logcosh: 1.5517e-05 - acc: 0.3338 - val_loss: 2.5995e-04 - val_mse: 2.5995e-04 - val_logcosh: 1.2981e-04 - val_acc: 0.3293\n",
      "Epoch 132/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.0755e-05 - mse: 3.0755e-05 - logcosh: 1.5379e-05 - acc: 0.3338 - val_loss: 2.3754e-04 - val_mse: 2.3754e-04 - val_logcosh: 1.1861e-04 - val_acc: 0.3293\n",
      "Epoch 133/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 2.9862e-05 - mse: 2.9862e-05 - logcosh: 1.4933e-05 - acc: 0.3338 - val_loss: 2.4761e-04 - val_mse: 2.4761e-04 - val_logcosh: 1.2363e-04 - val_acc: 0.3293\n",
      "Epoch 134/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.9745e-05 - mse: 2.9745e-05 - logcosh: 1.4876e-05 - acc: 0.3338 - val_loss: 2.1082e-04 - val_mse: 2.1082e-04 - val_logcosh: 1.0528e-04 - val_acc: 0.3293\n",
      "Epoch 135/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.0741e-05 - mse: 3.0741e-05 - logcosh: 1.5373e-05 - acc: 0.3338 - val_loss: 2.2107e-04 - val_mse: 2.2107e-04 - val_logcosh: 1.1039e-04 - val_acc: 0.3293\n",
      "Epoch 136/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.1333e-05 - mse: 3.1333e-05 - logcosh: 1.5669e-05 - acc: 0.3338 - val_loss: 2.2308e-04 - val_mse: 2.2308e-04 - val_logcosh: 1.1139e-04 - val_acc: 0.3293\n",
      "Epoch 137/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.9320e-05 - mse: 2.9320e-05 - logcosh: 1.4663e-05 - acc: 0.3338 - val_loss: 2.3084e-04 - val_mse: 2.3084e-04 - val_logcosh: 1.1527e-04 - val_acc: 0.3293\n",
      "Epoch 138/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 2.7329e-05 - mse: 2.7329e-05 - logcosh: 1.3667e-05 - acc: 0.3338 - val_loss: 2.3964e-04 - val_mse: 2.3964e-04 - val_logcosh: 1.1965e-04 - val_acc: 0.3293\n",
      "Epoch 139/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 2.9356e-05 - mse: 2.9356e-05 - logcosh: 1.4681e-05 - acc: 0.3338 - val_loss: 2.2578e-04 - val_mse: 2.2578e-04 - val_logcosh: 1.1275e-04 - val_acc: 0.3293\n",
      "Epoch 140/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.9102e-05 - mse: 2.9102e-05 - logcosh: 1.4553e-05 - acc: 0.3338 - val_loss: 2.1845e-04 - val_mse: 2.1845e-04 - val_logcosh: 1.0909e-04 - val_acc: 0.3293\n",
      "Epoch 141/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.1098e-05 - mse: 3.1098e-05 - logcosh: 1.5551e-05 - acc: 0.3338 - val_loss: 2.5628e-04 - val_mse: 2.5628e-04 - val_logcosh: 1.2796e-04 - val_acc: 0.3293\n",
      "Epoch 142/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 2.9474e-05 - mse: 2.9474e-05 - logcosh: 1.4739e-05 - acc: 0.3338 - val_loss: 2.2304e-04 - val_mse: 2.2304e-04 - val_logcosh: 1.1138e-04 - val_acc: 0.3293\n",
      "Epoch 143/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.8447e-05 - mse: 2.8447e-05 - logcosh: 1.4226e-05 - acc: 0.3338 - val_loss: 2.3770e-04 - val_mse: 2.3770e-04 - val_logcosh: 1.1869e-04 - val_acc: 0.3293\n",
      "Epoch 144/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 2.8264e-05 - mse: 2.8264e-05 - logcosh: 1.4134e-05 - acc: 0.3338 - val_loss: 2.3505e-04 - val_mse: 2.3505e-04 - val_logcosh: 1.1736e-04 - val_acc: 0.3293\n",
      "Epoch 145/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 2.7856e-05 - mse: 2.7856e-05 - logcosh: 1.3930e-05 - acc: 0.3338 - val_loss: 2.1999e-04 - val_mse: 2.1999e-04 - val_logcosh: 1.0986e-04 - val_acc: 0.3293\n",
      "Epoch 146/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.8548e-05 - mse: 2.8548e-05 - logcosh: 1.4276e-05 - acc: 0.3338 - val_loss: 2.2187e-04 - val_mse: 2.2187e-04 - val_logcosh: 1.1078e-04 - val_acc: 0.3293\n",
      "Epoch 147/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.1914e-05 - mse: 3.1914e-05 - logcosh: 1.5959e-05 - acc: 0.3338 - val_loss: 2.3785e-04 - val_mse: 2.3785e-04 - val_logcosh: 1.1877e-04 - val_acc: 0.3293\n",
      "Epoch 148/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.1024e-05 - mse: 3.1024e-05 - logcosh: 1.5514e-05 - acc: 0.3338 - val_loss: 2.5215e-04 - val_mse: 2.5215e-04 - val_logcosh: 1.2590e-04 - val_acc: 0.3293\n",
      "Epoch 149/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.9706e-05 - mse: 2.9706e-05 - logcosh: 1.4855e-05 - acc: 0.3338 - val_loss: 2.4395e-04 - val_mse: 2.4395e-04 - val_logcosh: 1.2180e-04 - val_acc: 0.3293\n",
      "Epoch 150/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.6939e-05 - mse: 2.6939e-05 - logcosh: 1.3473e-05 - acc: 0.3338 - val_loss: 2.3348e-04 - val_mse: 2.3348e-04 - val_logcosh: 1.1658e-04 - val_acc: 0.3293\n",
      "Epoch 151/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 2.8472e-05 - mse: 2.8472e-05 - logcosh: 1.4238e-05 - acc: 0.3338 - val_loss: 2.2126e-04 - val_mse: 2.2126e-04 - val_logcosh: 1.1048e-04 - val_acc: 0.3293\n",
      "Epoch 152/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 3.0519e-05 - mse: 3.0519e-05 - logcosh: 1.5261e-05 - acc: 0.3338 - val_loss: 2.4578e-04 - val_mse: 2.4578e-04 - val_logcosh: 1.2271e-04 - val_acc: 0.3293\n",
      "Epoch 153/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 3.0353e-05 - mse: 3.0353e-05 - logcosh: 1.5178e-05 - acc: 0.3338 - val_loss: 2.1357e-04 - val_mse: 2.1357e-04 - val_logcosh: 1.0665e-04 - val_acc: 0.3293\n",
      "Epoch 154/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.6268e-05 - mse: 2.6268e-05 - logcosh: 1.3137e-05 - acc: 0.3338 - val_loss: 2.0895e-04 - val_mse: 2.0895e-04 - val_logcosh: 1.0435e-04 - val_acc: 0.3293\n",
      "Epoch 155/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.7728e-05 - mse: 2.7728e-05 - logcosh: 1.3867e-05 - acc: 0.3338 - val_loss: 1.9220e-04 - val_mse: 1.9220e-04 - val_logcosh: 9.5976e-05 - val_acc: 0.3293\n",
      "Epoch 156/200\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 2.7952e-05 - mse: 2.7952e-05 - logcosh: 1.3979e-05 - acc: 0.3338 - val_loss: 2.1037e-04 - val_mse: 2.1037e-04 - val_logcosh: 1.0505e-04 - val_acc: 0.3293\n",
      "Epoch 157/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.7701e-05 - mse: 2.7701e-05 - logcosh: 1.3853e-05 - acc: 0.3338 - val_loss: 2.0394e-04 - val_mse: 2.0394e-04 - val_logcosh: 1.0185e-04 - val_acc: 0.3293\n",
      "Epoch 158/200\n",
      "422/422 [==============================] - 39s 91ms/step - loss: 2.9287e-05 - mse: 2.9287e-05 - logcosh: 1.4646e-05 - acc: 0.3338 - val_loss: 2.0963e-04 - val_mse: 2.0963e-04 - val_logcosh: 1.0469e-04 - val_acc: 0.3293\n",
      "Epoch 159/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.6779e-05 - mse: 2.6779e-05 - logcosh: 1.3392e-05 - acc: 0.3338 - val_loss: 2.1861e-04 - val_mse: 2.1861e-04 - val_logcosh: 1.0916e-04 - val_acc: 0.3293\n",
      "Epoch 160/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.7925e-05 - mse: 2.7925e-05 - logcosh: 1.3965e-05 - acc: 0.3338 - val_loss: 2.2042e-04 - val_mse: 2.2042e-04 - val_logcosh: 1.1008e-04 - val_acc: 0.3293\n",
      "Epoch 161/200\n",
      "422/422 [==============================] - 39s 91ms/step - loss: 2.6366e-05 - mse: 2.6366e-05 - logcosh: 1.3186e-05 - acc: 0.3338 - val_loss: 2.0954e-04 - val_mse: 2.0954e-04 - val_logcosh: 1.0465e-04 - val_acc: 0.3293\n",
      "Epoch 162/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.7114e-05 - mse: 2.7114e-05 - logcosh: 1.3559e-05 - acc: 0.3338 - val_loss: 2.0528e-04 - val_mse: 2.0528e-04 - val_logcosh: 1.0251e-04 - val_acc: 0.3293\n",
      "Epoch 163/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.7953e-05 - mse: 2.7953e-05 - logcosh: 1.3979e-05 - acc: 0.3338 - val_loss: 1.9794e-04 - val_mse: 1.9794e-04 - val_logcosh: 9.8859e-05 - val_acc: 0.3293\n",
      "Epoch 164/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.6179e-05 - mse: 2.6179e-05 - logcosh: 1.3093e-05 - acc: 0.3338 - val_loss: 2.0367e-04 - val_mse: 2.0367e-04 - val_logcosh: 1.0171e-04 - val_acc: 0.3293\n",
      "Epoch 165/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.7284e-05 - mse: 2.7284e-05 - logcosh: 1.3644e-05 - acc: 0.3338 - val_loss: 2.1008e-04 - val_mse: 2.1008e-04 - val_logcosh: 1.0491e-04 - val_acc: 0.3293\n",
      "Epoch 166/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.7848e-05 - mse: 2.7848e-05 - logcosh: 1.3927e-05 - acc: 0.3338 - val_loss: 2.1273e-04 - val_mse: 2.1273e-04 - val_logcosh: 1.0622e-04 - val_acc: 0.3293\n",
      "Epoch 167/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.7286e-05 - mse: 2.7286e-05 - logcosh: 1.3646e-05 - acc: 0.3338 - val_loss: 2.1106e-04 - val_mse: 2.1106e-04 - val_logcosh: 1.0539e-04 - val_acc: 0.3293\n",
      "Epoch 168/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.7059e-05 - mse: 2.7059e-05 - logcosh: 1.3532e-05 - acc: 0.3338 - val_loss: 2.5994e-04 - val_mse: 2.5994e-04 - val_logcosh: 1.2977e-04 - val_acc: 0.3293\n",
      "Epoch 169/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.7631e-05 - mse: 2.7631e-05 - logcosh: 1.3818e-05 - acc: 0.3338 - val_loss: 2.1427e-04 - val_mse: 2.1427e-04 - val_logcosh: 1.0700e-04 - val_acc: 0.3293\n",
      "Epoch 170/200\n",
      "422/422 [==============================] - 39s 91ms/step - loss: 2.7866e-05 - mse: 2.7866e-05 - logcosh: 1.3935e-05 - acc: 0.3338 - val_loss: 2.0218e-04 - val_mse: 2.0218e-04 - val_logcosh: 1.0096e-04 - val_acc: 0.3293\n",
      "Epoch 171/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.7189e-05 - mse: 2.7189e-05 - logcosh: 1.3597e-05 - acc: 0.3338 - val_loss: 2.2978e-04 - val_mse: 2.2978e-04 - val_logcosh: 1.1473e-04 - val_acc: 0.3293\n",
      "Epoch 172/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.5659e-05 - mse: 2.5659e-05 - logcosh: 1.2832e-05 - acc: 0.3338 - val_loss: 2.3064e-04 - val_mse: 2.3064e-04 - val_logcosh: 1.1517e-04 - val_acc: 0.3293\n",
      "Epoch 173/200\n",
      "422/422 [==============================] - 39s 93ms/step - loss: 2.6097e-05 - mse: 2.6097e-05 - logcosh: 1.3051e-05 - acc: 0.3338 - val_loss: 1.9203e-04 - val_mse: 1.9203e-04 - val_logcosh: 9.5893e-05 - val_acc: 0.3293\n",
      "Epoch 174/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 3.1399e-05 - mse: 3.1399e-05 - logcosh: 1.5701e-05 - acc: 0.3338 - val_loss: 1.7996e-04 - val_mse: 1.7996e-04 - val_logcosh: 8.9870e-05 - val_acc: 0.3293\n",
      "Epoch 175/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.6780e-05 - mse: 2.6780e-05 - logcosh: 1.3393e-05 - acc: 0.3338 - val_loss: 1.7981e-04 - val_mse: 1.7981e-04 - val_logcosh: 8.9793e-05 - val_acc: 0.3293\n",
      "Epoch 176/200\n",
      "422/422 [==============================] - 39s 91ms/step - loss: 2.6356e-05 - mse: 2.6356e-05 - logcosh: 1.3180e-05 - acc: 0.3338 - val_loss: 2.1590e-04 - val_mse: 2.1590e-04 - val_logcosh: 1.0779e-04 - val_acc: 0.3293\n",
      "Epoch 177/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.5715e-05 - mse: 2.5715e-05 - logcosh: 1.2860e-05 - acc: 0.3338 - val_loss: 2.0480e-04 - val_mse: 2.0480e-04 - val_logcosh: 1.0226e-04 - val_acc: 0.3293\n",
      "Epoch 178/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.7277e-05 - mse: 2.7277e-05 - logcosh: 1.3641e-05 - acc: 0.3338 - val_loss: 1.9201e-04 - val_mse: 1.9201e-04 - val_logcosh: 9.5877e-05 - val_acc: 0.3293\n",
      "Epoch 179/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.9750e-05 - mse: 2.9750e-05 - logcosh: 1.4877e-05 - acc: 0.3338 - val_loss: 2.0527e-04 - val_mse: 2.0527e-04 - val_logcosh: 1.0251e-04 - val_acc: 0.3293\n",
      "Epoch 180/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.8815e-05 - mse: 2.8815e-05 - logcosh: 1.4407e-05 - acc: 0.3338 - val_loss: 1.7633e-04 - val_mse: 1.7633e-04 - val_logcosh: 8.8059e-05 - val_acc: 0.3293\n",
      "Epoch 181/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.6874e-05 - mse: 2.6874e-05 - logcosh: 1.3439e-05 - acc: 0.3338 - val_loss: 2.0425e-04 - val_mse: 2.0425e-04 - val_logcosh: 1.0200e-04 - val_acc: 0.3293\n",
      "Epoch 182/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.4476e-05 - mse: 2.4476e-05 - logcosh: 1.2241e-05 - acc: 0.3338 - val_loss: 2.2270e-04 - val_mse: 2.2270e-04 - val_logcosh: 1.1119e-04 - val_acc: 0.3293\n",
      "Epoch 183/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.5780e-05 - mse: 2.5780e-05 - logcosh: 1.2893e-05 - acc: 0.3338 - val_loss: 1.9990e-04 - val_mse: 1.9990e-04 - val_logcosh: 9.9837e-05 - val_acc: 0.3293\n",
      "Epoch 184/200\n",
      "422/422 [==============================] - 39s 92ms/step - loss: 2.5531e-05 - mse: 2.5531e-05 - logcosh: 1.2768e-05 - acc: 0.3338 - val_loss: 1.8974e-04 - val_mse: 1.8974e-04 - val_logcosh: 9.4766e-05 - val_acc: 0.3293\n",
      "Epoch 185/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.3981e-05 - mse: 2.3981e-05 - logcosh: 1.1993e-05 - acc: 0.3338 - val_loss: 2.0702e-04 - val_mse: 2.0702e-04 - val_logcosh: 1.0338e-04 - val_acc: 0.3293\n",
      "Epoch 186/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.4941e-05 - mse: 2.4941e-05 - logcosh: 1.2474e-05 - acc: 0.3338 - val_loss: 2.0519e-04 - val_mse: 2.0519e-04 - val_logcosh: 1.0247e-04 - val_acc: 0.3293\n",
      "Epoch 187/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.5789e-05 - mse: 2.5789e-05 - logcosh: 1.2897e-05 - acc: 0.3338 - val_loss: 1.8754e-04 - val_mse: 1.8754e-04 - val_logcosh: 9.3646e-05 - val_acc: 0.3293\n",
      "Epoch 188/200\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 2.4173e-05 - mse: 2.4173e-05 - logcosh: 1.2090e-05 - acc: 0.3338 - val_loss: 2.0058e-04 - val_mse: 2.0058e-04 - val_logcosh: 1.0015e-04 - val_acc: 0.3293\n",
      "Epoch 189/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.3679e-05 - mse: 2.3679e-05 - logcosh: 1.1843e-05 - acc: 0.3338 - val_loss: 1.7354e-04 - val_mse: 1.7354e-04 - val_logcosh: 8.6670e-05 - val_acc: 0.3293\n",
      "Epoch 190/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.5591e-05 - mse: 2.5591e-05 - logcosh: 1.2798e-05 - acc: 0.3338 - val_loss: 2.6964e-04 - val_mse: 2.6964e-04 - val_logcosh: 1.3462e-04 - val_acc: 0.3293\n",
      "Epoch 191/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.7798e-05 - mse: 2.7798e-05 - logcosh: 1.3902e-05 - acc: 0.3338 - val_loss: 2.0067e-04 - val_mse: 2.0067e-04 - val_logcosh: 1.0021e-04 - val_acc: 0.3293\n",
      "Epoch 192/200\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 2.5712e-05 - mse: 2.5712e-05 - logcosh: 1.2859e-05 - acc: 0.3338 - val_loss: 1.9971e-04 - val_mse: 1.9971e-04 - val_logcosh: 9.9721e-05 - val_acc: 0.3293\n",
      "Epoch 193/200\n",
      "422/422 [==============================] - 38s 91ms/step - loss: 2.7988e-05 - mse: 2.7988e-05 - logcosh: 1.3996e-05 - acc: 0.3338 - val_loss: 2.3048e-04 - val_mse: 2.3048e-04 - val_logcosh: 1.1509e-04 - val_acc: 0.3293\n",
      "Epoch 194/200\n",
      "232/422 [===============>..............] - ETA: 16s - loss: 2.5353e-05 - mse: 2.5353e-05 - logcosh: 1.2678e-05 - acc: 0.3322"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_15892/2982525251.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdatas_list_wear\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mwear_label\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m200\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_split\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1382\u001B[0m                 _r=1):\n\u001B[0;32m   1383\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1384\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1385\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1386\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mtrain_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m   1019\u001B[0m       \u001B[1;32mdef\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1020\u001B[0m         \u001B[1;34m\"\"\"Runs a training execution with a single step.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1021\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mstep_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1022\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1023\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun_eagerly\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mstep_function\u001B[1;34m(model, iterator)\u001B[0m\n\u001B[0;32m   1008\u001B[0m             run_step, jit_compile=True, experimental_relax_shapes=True)\n\u001B[0;32m   1009\u001B[0m       \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1010\u001B[1;33m       \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistribute_strategy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1011\u001B[0m       outputs = reduce_per_replica(\n\u001B[0;32m   1012\u001B[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   1310\u001B[0m       fn = autograph.tf_convert(\n\u001B[0;32m   1311\u001B[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001B[1;32m-> 1312\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_extended\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall_for_each_replica\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1313\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1314\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mreduce\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce_op\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36mcall_for_each_replica\u001B[1;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[0;32m   2886\u001B[0m       \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2887\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_container_strategy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2888\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_for_each_replica\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2889\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2890\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_for_each_replica\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36m_call_for_each_replica\u001B[1;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[0;32m   3687\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_for_each_replica\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3688\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mReplicaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_container_strategy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreplica_id_in_sync_group\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3689\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3690\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3691\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_reduce_to\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce_op\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdestinations\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    593\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    594\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mControlStatusCtx\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstatus\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mStatus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mUNSPECIFIED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 595\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    596\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    597\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mismethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mrun_step\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    998\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    999\u001B[0m       \u001B[1;32mdef\u001B[0m \u001B[0mrun_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1000\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1001\u001B[0m         \u001B[1;31m# Ensure counter is updated only if `train_step` succeeds.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1002\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontrol_dependencies\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_minimum_control_deps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mtrain_step\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    861\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_target_and_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    862\u001B[0m     \u001B[1;31m# Run backwards pass.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 863\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mminimize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    864\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_metrics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    865\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001B[0m in \u001B[0;36mminimize\u001B[1;34m(self, loss, var_list, grad_loss, name, tape)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m     \"\"\"\n\u001B[1;32m--> 530\u001B[1;33m     grads_and_vars = self._compute_gradients(\n\u001B[0m\u001B[0;32m    531\u001B[0m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001B[0;32m    532\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_gradients\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrads_and_vars\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001B[0m in \u001B[0;36m_compute_gradients\u001B[1;34m(self, loss, var_list, grad_loss, tape)\u001B[0m\n\u001B[0;32m    581\u001B[0m     \u001B[0mvar_list\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvar_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    582\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"/gradients\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 583\u001B[1;33m       \u001B[0mgrads_and_vars\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_gradients\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvar_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    584\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m     self._assert_valid_dtypes([\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001B[0m in \u001B[0;36m_get_gradients\u001B[1;34m(self, tape, loss, var_list, grad_loss)\u001B[0m\n\u001B[0;32m    462\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_get_gradients\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvar_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_loss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    463\u001B[0m     \u001B[1;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 464\u001B[1;33m     \u001B[0mgrads\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvar_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    465\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrads\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvar_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    466\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001B[0m in \u001B[0;36mgradient\u001B[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[0;32m   1079\u001B[0m                           for x in nest.flatten(output_gradients)]\n\u001B[0;32m   1080\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1081\u001B[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001B[0m\u001B[0;32m   1082\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1083\u001B[0m         \u001B[0mflat_targets\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001B[0m in \u001B[0;36mimperative_grad\u001B[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[0;32m     65\u001B[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001B[0m\u001B[0;32m     68\u001B[0m       \u001B[0mtape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001B[0m in \u001B[0;36m_gradient_function\u001B[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[0;32m    154\u001B[0m       \u001B[0mgradient_name_scope\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mforward_pass_name_scope\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"/\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgradient_name_scope\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 156\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    157\u001B[0m   \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    158\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001B[0m in \u001B[0;36m_SumGrad\u001B[1;34m(op, grad)\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtile_scaling\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 205\u001B[1;33m   \u001B[0minput_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    206\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    207\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_attr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"keep_dims\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mop_dispatch_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1080\u001B[0m       \u001B[1;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1081\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1082\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1083\u001B[0m       \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1084\u001B[0m         \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36mshape\u001B[1;34m(input, name, out_type)\u001B[0m\n\u001B[0;32m    647\u001B[0m     \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0mof\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mout_type\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    648\u001B[0m   \"\"\"\n\u001B[1;32m--> 649\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mshape_internal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    650\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    651\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36mshape_internal\u001B[1;34m(input, name, optimize, out_type)\u001B[0m\n\u001B[0;32m    688\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mout_type\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    689\u001B[0m         \u001B[0mout_type\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mint32\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 690\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    691\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    692\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001B[0m in \u001B[0;36mshape\u001B[1;34m(input, out_type, name)\u001B[0m\n\u001B[0;32m   9346\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9347\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 9348\u001B[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[0;32m   9349\u001B[0m         _ctx, \"Shape\", name, input, \"out_type\", out_type)\n\u001B[0;32m   9350\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x=datas_list_wear, y=wear_label, batch_size=32, epochs=200, validation_split=0.1, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# history.history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00064721]] 0.0\n",
      "[[0.06380904]] 0.0625\n",
      "[[0.06187547]] 0.0625\n",
      "[[0.21103278]] 0.2017\n",
      "[[0.2082099]] 0.2017\n",
      "[[0.06102732]] 0.0625\n",
      "[[0.06799102]] 0.0625\n",
      "[[0.06195023]] 0.0625\n",
      "[[0.06344761]] 0.0625\n",
      "[[0.06257687]] 0.0625\n",
      "[[0.00053883]] 0.0\n",
      "[[0.21397778]] 0.2017\n",
      "[[0.06326868]] 0.0625\n",
      "[[-0.00157976]] 0.0\n",
      "[[-0.00226849]] 0.0\n",
      "[[0.0622728]] 0.0625\n",
      "[[-0.0015426]] 0.0\n",
      "[[0.06223111]] 0.0625\n",
      "[[0.20701215]] 0.2017\n",
      "[[0.06483253]] 0.0625\n",
      "[[0.0034482]] 0.0\n",
      "[[0.06312136]] 0.0625\n",
      "[[0.2059196]] 0.2017\n",
      "[[0.06327878]] 0.0625\n",
      "[[0.06255399]] 0.0625\n",
      "[[0.05878639]] 0.0625\n",
      "[[0.06194175]] 0.0625\n",
      "[[0.21578294]] 0.2017\n",
      "[[0.06167879]] 0.0625\n",
      "[[0.21229514]] 0.2017\n",
      "[[0.06465511]] 0.0625\n",
      "[[0.00073507]] 0.0\n",
      "[[0.00013443]] 0.0\n",
      "[[0.06177294]] 0.0625\n",
      "[[0.00313608]] 0.0\n",
      "[[0.20373923]] 0.2017\n",
      "[[0.00120271]] 0.0\n",
      "[[0.00017727]] 0.0\n",
      "[[0.21123722]] 0.2017\n",
      "[[0.00138832]] 0.0\n",
      "[[0.20643458]] 0.2017\n",
      "[[0.06363418]] 0.0625\n",
      "[[0.21215004]] 0.2017\n",
      "[[0.20833537]] 0.2017\n",
      "[[-0.00426971]] 0.0\n",
      "[[-0.00070954]] 0.0\n",
      "[[0.20080519]] 0.2017\n",
      "[[0.05968245]] 0.0625\n",
      "[[0.20874715]] 0.2017\n",
      "[[0.20438743]] 0.2017\n",
      "[[0.05949442]] 0.0625\n",
      "[[0.20633772]] 0.2017\n",
      "[[0.06349941]] 0.0625\n",
      "[[-0.00029106]] 0.0\n",
      "[[0.21468651]] 0.2017\n",
      "[[0.06184294]] 0.0625\n",
      "[[0.20859781]] 0.2017\n",
      "[[0.20726892]] 0.2017\n",
      "[[0.21216875]] 0.2017\n",
      "[[0.20915282]] 0.2017\n",
      "[[0.207086]] 0.2017\n",
      "[[0.20699197]] 0.2017\n",
      "[[0.00129115]] 0.0\n",
      "[[0.06400147]] 0.0625\n",
      "[[0.00263176]] 0.0\n",
      "[[-0.00058751]] 0.0\n",
      "[[0.21337563]] 0.2017\n",
      "[[0.0622449]] 0.0625\n",
      "[[0.06063656]] 0.0625\n",
      "[[0.06224885]] 0.0625\n",
      "[[0.00202562]] 0.0\n",
      "[[0.2093193]] 0.2017\n",
      "[[0.00235755]] 0.0\n",
      "[[0.20304716]] 0.2017\n",
      "[[0.06606406]] 0.0625\n",
      "[[0.20154408]] 0.2017\n",
      "[[0.20344588]] 0.2017\n",
      "[[-0.00013093]] 0.0\n",
      "[[0.00105552]] 0.0\n",
      "[[0.0625183]] 0.0625\n",
      "[[0.06343515]] 0.0625\n",
      "[[-0.00194049]] 0.0\n",
      "[[0.00142241]] 0.0\n",
      "[[0.0610145]] 0.0625\n",
      "[[0.06181129]] 0.0625\n",
      "[[0.21399218]] 0.2017\n",
      "[[0.20317015]] 0.2017\n",
      "[[0.00134563]] 0.0\n",
      "[[-0.00011458]] 0.0\n",
      "[[0.05311048]] 0.0625\n",
      "[[0.21075726]] 0.2017\n",
      "[[0.2061669]] 0.2017\n",
      "[[0.00253819]] 0.0\n",
      "[[0.06681609]] 0.0625\n",
      "[[0.20532373]] 0.2017\n",
      "[[0.00198397]] 0.0\n",
      "[[0.0598246]] 0.0625\n",
      "[[0.06220037]] 0.0625\n",
      "[[0.0650115]] 0.0625\n",
      "[[0.00640989]] 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(model.predict(datas_list_wear[i].reshape(1, -1, 3)), wear_label[i])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08212824]]\n",
      "[[0.0522954]]\n",
      "[[0.07301395]]\n",
      "[[0.07820825]]\n",
      "[[0.06283331]]\n",
      "[[0.06406403]]\n",
      "[[0.06777911]]\n",
      "[[0.06455211]]\n",
      "[[0.07579665]]\n",
      "[[0.06986951]]\n",
      "[[0.08775322]]\n",
      "[[0.0632422]]\n",
      "[[0.09669397]]\n",
      "[[0.07641236]]\n",
      "[[0.07420598]]\n",
      "[[0.07146553]]\n",
      "[[0.06070114]]\n",
      "[[0.07892074]]\n",
      "[[0.06806369]]\n",
      "[[0.06150125]]\n",
      "[[0.07769336]]\n",
      "[[0.06059061]]\n",
      "[[0.09019547]]\n",
      "[[0.07649443]]\n",
      "[[0.08315217]]\n",
      "[[0.06486537]]\n",
      "[[0.06987017]]\n",
      "[[0.07343129]]\n",
      "[[0.08368061]]\n",
      "[[0.07126123]]\n",
      "[[0.07489692]]\n",
      "[[0.07833502]]\n",
      "[[0.06856854]]\n",
      "[[0.07075918]]\n",
      "[[0.07857106]]\n",
      "[[0.07242362]]\n",
      "[[0.06632741]]\n",
      "[[0.08786847]]\n",
      "[[0.06732018]]\n",
      "[[0.08485702]]\n",
      "[[0.06224223]]\n",
      "[[0.10218641]]\n",
      "[[0.0709579]]\n",
      "[[0.07377212]]\n",
      "[[0.08159497]]\n",
      "[[0.06952846]]\n",
      "[[0.06995212]]\n",
      "[[0.06690352]]\n",
      "[[0.07453748]]\n",
      "[[0.08749929]]\n",
      "[[0.07628587]]\n",
      "[[0.08982819]]\n",
      "[[0.05880275]]\n",
      "[[0.09065254]]\n",
      "[[0.07162903]]\n",
      "[[0.08429214]]\n",
      "[[0.07199755]]\n",
      "[[0.0745839]]\n",
      "[[0.07362768]]\n",
      "[[0.07536709]]\n",
      "[[0.07394406]]\n",
      "[[0.06682014]]\n",
      "[[0.0699711]]\n",
      "[[0.0721916]]\n",
      "[[0.07128799]]\n",
      "[[0.06840231]]\n",
      "[[0.07022358]]\n",
      "[[0.0817603]]\n",
      "[[0.07841479]]\n",
      "[[0.06869406]]\n",
      "[[0.0697457]]\n",
      "[[0.0747536]]\n",
      "[[0.07114367]]\n",
      "[[0.06996165]]\n",
      "[[0.07651731]]\n",
      "[[0.06893285]]\n",
      "[[0.06600785]]\n",
      "[[0.07351368]]\n",
      "[[0.08269318]]\n",
      "[[0.07317591]]\n",
      "[[0.07613384]]\n",
      "[[0.06735682]]\n",
      "[[0.09050616]]\n",
      "[[0.08558528]]\n",
      "[[0.0643449]]\n",
      "[[0.07793175]]\n",
      "[[0.07472541]]\n",
      "[[0.08293919]]\n",
      "[[0.08525107]]\n",
      "[[0.07017522]]\n",
      "[[0.06923794]]\n",
      "[[0.0768199]]\n",
      "[[0.07516888]]\n",
      "[[0.07567044]]\n",
      "[[0.05799539]]\n",
      "[[0.08403005]]\n",
      "[[0.07850029]]\n",
      "[[0.08876188]]\n",
      "[[0.07717847]]\n",
      "[[0.07901554]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # print(model.predict(wear_top[i].reshape(1, -1, 3)))\n",
    "    print(model.predict(wear_mid[i].reshape(1, -1, 3)))\n",
    "    # print(model.predict(wear_bot[i].reshape(1, -1, 3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}